{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b2fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report, confusion_matrix, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, learning_curve\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f27614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    return sentence.lower()\n",
    "\n",
    "data = pd.read_csv('Data-with-Label.csv')\n",
    "data['prompt'] = data['prompt'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fe092c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.52798457]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.70710678]\n",
      " [0.         0.         0.         ... 0.         0.         0.70710678]\n",
      " [0.         0.         0.         ... 0.         0.         0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words='english')\n",
    "X = vectorizer.fit_transform(data['prompt']).toarray()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7, stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(data['prompt']).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4017261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "# Create a list of labels\n",
    "labels = ['special', 'chat',]\n",
    "\n",
    "# Create an instance of LabelEncoder\n",
    "le = LabelEncoder()\n",
    "yNgu = le.fit_transform(labels)\n",
    "\n",
    "z = data['category'].map({'special': 1, 'chat': 0})\n",
    "y = z\n",
    "filtered_array = [num for num in z if num == 1]\n",
    "print(yNgu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c0d5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c7561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa299dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44     0\n",
      "47     1\n",
      "4      0\n",
      "55     0\n",
      "26     1\n",
      "64     0\n",
      "73     0\n",
      "10     1\n",
      "40     0\n",
      "107    1\n",
      "18     1\n",
      "62     1\n",
      "11     0\n",
      "36     0\n",
      "89     0\n",
      "91     0\n",
      "109    1\n",
      "0      0\n",
      "88     0\n",
      "104    0\n",
      "65     0\n",
      "45     0\n",
      "31     0\n",
      "70     0\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# classifier = SGDClassifier(max_iter=1600)\n",
    "# classifier.fit(X_train, y_train)\n",
    "print(y_test)\n",
    "classifier = SGDClassifier(max_iter=1, warm_start=True)\n",
    "for i in range(1600):\n",
    "    classifier.partial_fit(X_train, y_train, classes=np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aaf25d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy1: 0.75\n",
      "Confusion Matrix:\n",
      " [[17  0]\n",
      " [ 6  1]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85        17\n",
      "           1       1.00      0.14      0.25         7\n",
      "\n",
      "    accuracy                           0.75        24\n",
      "   macro avg       0.87      0.57      0.55        24\n",
      "weighted avg       0.82      0.75      0.67        24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "myAccuracy = accuracy_score(y_test, y_pred)\n",
    "myConfusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "myClassification_report = classification_report(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Accuracy1:\", myAccuracy)\n",
    "print(\"Confusion Matrix:\\n\", myConfusion_matrix)\n",
    "print(\"Classification Report:\\n\", myClassification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e921505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(MLPClassifier(hidden_layer_sizes=(30,), max_iter=1600), X_train, y_train, cv=5)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.title(\"Learning Curve with MLPClassifier\")\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylim(0.0, 1.1)\n",
    "lw = 2\n",
    "plt.plot(train_sizes, train_scores_mean, label=\"Training score\",\n",
    "             color=\"darkorange\", lw=lw)\n",
    "plt.plot(train_sizes, test_scores_mean, label=\"Test score\",\n",
    "             color=\"navy\", lw=lw)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3ef16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0b1cc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1/1600\n",
      "Iteration 1, loss = 0.74746739\n",
      "Training epoch 2/1600\n",
      "Iteration 2, loss = 0.71126821\n",
      "Training epoch 3/1600\n",
      "Iteration 3, loss = 0.67469957\n",
      "Training epoch 4/1600\n",
      "Iteration 4, loss = 0.64495294\n",
      "Training epoch 5/1600\n",
      "Iteration 5, loss = 0.62475097\n",
      "Training epoch 6/1600\n",
      "Iteration 6, loss = 0.61449232\n",
      "Training epoch 7/1600\n",
      "Iteration 7, loss = 0.61078784\n",
      "Training epoch 8/1600\n",
      "Iteration 8, loss = 0.60986989\n",
      "Training epoch 9/1600\n",
      "Iteration 9, loss = 0.60838344\n",
      "Training epoch 10/1600\n",
      "Iteration 10, loss = 0.60430780\n",
      "Training epoch 11/1600\n",
      "Iteration 11, loss = 0.59737075\n",
      "Training epoch 12/1600\n",
      "Iteration 12, loss = 0.58889344\n",
      "Training epoch 13/1600\n",
      "Iteration 13, loss = 0.57982923\n",
      "Training epoch 14/1600\n",
      "Iteration 14, loss = 0.57099954\n",
      "Training epoch 15/1600\n",
      "Iteration 15, loss = 0.56303876\n",
      "Training epoch 16/1600\n",
      "Iteration 16, loss = 0.55583106\n",
      "Training epoch 17/1600\n",
      "Iteration 17, loss = 0.54922319\n",
      "Training epoch 18/1600\n",
      "Iteration 18, loss = 0.54240803\n",
      "Training epoch 19/1600\n",
      "Iteration 19, loss = 0.53491753\n",
      "Training epoch 20/1600\n",
      "Iteration 20, loss = 0.52668969\n",
      "Training epoch 21/1600\n",
      "Iteration 21, loss = 0.51766976\n",
      "Training epoch 22/1600\n",
      "Iteration 22, loss = 0.50825932\n",
      "Training epoch 23/1600\n",
      "Iteration 23, loss = 0.49873854\n",
      "Training epoch 24/1600\n",
      "Iteration 24, loss = 0.48923589\n",
      "Training epoch 25/1600\n",
      "Iteration 25, loss = 0.47982566\n",
      "Training epoch 26/1600\n",
      "Iteration 26, loss = 0.47057071\n",
      "Training epoch 27/1600\n",
      "Iteration 27, loss = 0.46142372\n",
      "Training epoch 28/1600\n",
      "Iteration 28, loss = 0.45229697\n",
      "Training epoch 29/1600\n",
      "Iteration 29, loss = 0.44318904\n",
      "Training epoch 30/1600\n",
      "Iteration 30, loss = 0.43407173\n",
      "Training epoch 31/1600\n",
      "Iteration 31, loss = 0.42508678\n",
      "Training epoch 32/1600\n",
      "Iteration 32, loss = 0.41631310\n",
      "Training epoch 33/1600\n",
      "Iteration 33, loss = 0.40782625\n",
      "Training epoch 34/1600\n",
      "Iteration 34, loss = 0.39978465\n",
      "Training epoch 35/1600\n",
      "Iteration 35, loss = 0.39209896\n",
      "Training epoch 36/1600\n",
      "Iteration 36, loss = 0.38476944\n",
      "Training epoch 37/1600\n",
      "Iteration 37, loss = 0.37792683\n",
      "Training epoch 38/1600\n",
      "Iteration 38, loss = 0.37150733\n",
      "Training epoch 39/1600\n",
      "Iteration 39, loss = 0.36549970\n",
      "Training epoch 40/1600\n",
      "Iteration 40, loss = 0.35986397\n",
      "Training epoch 41/1600\n",
      "Iteration 41, loss = 0.35459734\n",
      "Training epoch 42/1600\n",
      "Iteration 42, loss = 0.34967629\n",
      "Training epoch 43/1600\n",
      "Iteration 43, loss = 0.34512917\n",
      "Training epoch 44/1600\n",
      "Iteration 44, loss = 0.34105925\n",
      "Training epoch 45/1600\n",
      "Iteration 45, loss = 0.33730163\n",
      "Training epoch 46/1600\n",
      "Iteration 46, loss = 0.33381563\n",
      "Training epoch 47/1600\n",
      "Iteration 47, loss = 0.33056994\n",
      "Training epoch 48/1600\n",
      "Iteration 48, loss = 0.32755650\n",
      "Training epoch 49/1600\n",
      "Iteration 49, loss = 0.32473574\n",
      "Training epoch 50/1600\n",
      "Iteration 50, loss = 0.32206025\n",
      "Training epoch 51/1600\n",
      "Iteration 51, loss = 0.31950697\n",
      "Training epoch 52/1600\n",
      "Iteration 52, loss = 0.31709088\n",
      "Training epoch 53/1600\n",
      "Iteration 53, loss = 0.31483852\n",
      "Training epoch 54/1600\n",
      "Iteration 54, loss = 0.31275957\n",
      "Training epoch 55/1600\n",
      "Iteration 55, loss = 0.31082062\n",
      "Training epoch 56/1600\n",
      "Iteration 56, loss = 0.30900557\n",
      "Training epoch 57/1600\n",
      "Iteration 57, loss = 0.30742415\n",
      "Training epoch 58/1600\n",
      "Iteration 58, loss = 0.30596385\n",
      "Training epoch 59/1600\n",
      "Iteration 59, loss = 0.30459126\n",
      "Training epoch 60/1600\n",
      "Iteration 60, loss = 0.30327235\n",
      "Training epoch 61/1600\n",
      "Iteration 61, loss = 0.30199422\n",
      "Training epoch 62/1600\n",
      "Iteration 62, loss = 0.30075925\n",
      "Training epoch 63/1600\n",
      "Iteration 63, loss = 0.29954694\n",
      "Training epoch 64/1600\n",
      "Iteration 64, loss = 0.29835660\n",
      "Training epoch 65/1600\n",
      "Iteration 65, loss = 0.29722651\n",
      "Training epoch 66/1600\n",
      "Iteration 66, loss = 0.29611982\n",
      "Training epoch 67/1600\n",
      "Iteration 67, loss = 0.29509068\n",
      "Training epoch 68/1600\n",
      "Iteration 68, loss = 0.29408102\n",
      "Training epoch 69/1600\n",
      "Iteration 69, loss = 0.29310573\n",
      "Training epoch 70/1600\n",
      "Iteration 70, loss = 0.29221442\n",
      "Training epoch 71/1600\n",
      "Iteration 71, loss = 0.29137493\n",
      "Training epoch 72/1600\n",
      "Iteration 72, loss = 0.29059327\n",
      "Training epoch 73/1600\n",
      "Iteration 73, loss = 0.28987228\n",
      "Training epoch 74/1600\n",
      "Iteration 74, loss = 0.28916532\n",
      "Training epoch 75/1600\n",
      "Iteration 75, loss = 0.28846686\n",
      "Training epoch 76/1600\n",
      "Iteration 76, loss = 0.28779593\n",
      "Training epoch 77/1600\n",
      "Iteration 77, loss = 0.28712726\n",
      "Training epoch 78/1600\n",
      "Iteration 78, loss = 0.28646336\n",
      "Training epoch 79/1600\n",
      "Iteration 79, loss = 0.28581069\n",
      "Training epoch 80/1600\n",
      "Iteration 80, loss = 0.28516998\n",
      "Training epoch 81/1600\n",
      "Iteration 81, loss = 0.28453341\n",
      "Training epoch 82/1600\n",
      "Iteration 82, loss = 0.28389848\n",
      "Training epoch 83/1600\n",
      "Iteration 83, loss = 0.28327326\n",
      "Training epoch 84/1600\n",
      "Iteration 84, loss = 0.28266237\n",
      "Training epoch 85/1600\n",
      "Iteration 85, loss = 0.28208642\n",
      "Training epoch 86/1600\n",
      "Iteration 86, loss = 0.28151393\n",
      "Training epoch 87/1600\n",
      "Iteration 87, loss = 0.28095090\n",
      "Training epoch 88/1600\n",
      "Iteration 88, loss = 0.28039560\n",
      "Training epoch 89/1600\n",
      "Iteration 89, loss = 0.27984393\n",
      "Training epoch 90/1600\n",
      "Iteration 90, loss = 0.27928267\n",
      "Training epoch 91/1600\n",
      "Iteration 91, loss = 0.27873170\n",
      "Training epoch 92/1600\n",
      "Iteration 92, loss = 0.27819803\n",
      "Training epoch 93/1600\n",
      "Iteration 93, loss = 0.27766462\n",
      "Training epoch 94/1600\n",
      "Iteration 94, loss = 0.27714078\n",
      "Training epoch 95/1600\n",
      "Iteration 95, loss = 0.27661777\n",
      "Training epoch 96/1600\n",
      "Iteration 96, loss = 0.27609026\n",
      "Training epoch 97/1600\n",
      "Iteration 97, loss = 0.27559402\n",
      "Training epoch 98/1600\n",
      "Iteration 98, loss = 0.27510336\n",
      "Training epoch 99/1600\n",
      "Iteration 99, loss = 0.27461327\n",
      "Training epoch 100/1600\n",
      "Iteration 100, loss = 0.27412985\n",
      "Training epoch 101/1600\n",
      "Iteration 101, loss = 0.27365151\n",
      "Training epoch 102/1600\n",
      "Iteration 102, loss = 0.27320042\n",
      "Training epoch 103/1600\n",
      "Iteration 103, loss = 0.27273603\n",
      "Training epoch 104/1600\n",
      "Iteration 104, loss = 0.27230304\n",
      "Training epoch 105/1600\n",
      "Iteration 105, loss = 0.27186514\n",
      "Training epoch 106/1600\n",
      "Iteration 106, loss = 0.27145595\n",
      "Training epoch 107/1600\n",
      "Iteration 107, loss = 0.27104063\n",
      "Training epoch 108/1600\n",
      "Iteration 108, loss = 0.27063856\n",
      "Training epoch 109/1600\n",
      "Iteration 109, loss = 0.27022990\n",
      "Training epoch 110/1600\n",
      "Iteration 110, loss = 0.26986649\n",
      "Training epoch 111/1600\n",
      "Iteration 111, loss = 0.26949969\n",
      "Training epoch 112/1600\n",
      "Iteration 112, loss = 0.26912647\n",
      "Training epoch 113/1600\n",
      "Iteration 113, loss = 0.26876583\n",
      "Training epoch 114/1600\n",
      "Iteration 114, loss = 0.26839167\n",
      "Training epoch 115/1600\n",
      "Iteration 115, loss = 0.26802903\n",
      "Training epoch 116/1600\n",
      "Iteration 116, loss = 0.26766220\n",
      "Training epoch 117/1600\n",
      "Iteration 117, loss = 0.26731907\n",
      "Training epoch 118/1600\n",
      "Iteration 118, loss = 0.26696114\n",
      "Training epoch 119/1600\n",
      "Iteration 119, loss = 0.26662152\n",
      "Training epoch 120/1600\n",
      "Iteration 120, loss = 0.26627634\n",
      "Training epoch 121/1600\n",
      "Iteration 121, loss = 0.26594649\n",
      "Training epoch 122/1600\n",
      "Iteration 122, loss = 0.26560559\n",
      "Training epoch 123/1600\n",
      "Iteration 123, loss = 0.26528618\n",
      "Training epoch 124/1600\n",
      "Iteration 124, loss = 0.26495607\n",
      "Training epoch 125/1600\n",
      "Iteration 125, loss = 0.26464089\n",
      "Training epoch 126/1600\n",
      "Iteration 126, loss = 0.26431967\n",
      "Training epoch 127/1600\n",
      "Iteration 127, loss = 0.26400892\n",
      "Training epoch 128/1600\n",
      "Iteration 128, loss = 0.26368968\n",
      "Training epoch 129/1600\n",
      "Iteration 129, loss = 0.26338520\n",
      "Training epoch 130/1600\n",
      "Iteration 130, loss = 0.26309861\n",
      "Training epoch 131/1600\n",
      "Iteration 131, loss = 0.26280640\n",
      "Training epoch 132/1600\n",
      "Iteration 132, loss = 0.26251694\n",
      "Training epoch 133/1600\n",
      "Iteration 133, loss = 0.26222449\n",
      "Training epoch 134/1600\n",
      "Iteration 134, loss = 0.26192824\n",
      "Training epoch 135/1600\n",
      "Iteration 135, loss = 0.26162463\n",
      "Training epoch 136/1600\n",
      "Iteration 136, loss = 0.26135044\n",
      "Training epoch 137/1600\n",
      "Iteration 137, loss = 0.26107641\n",
      "Training epoch 138/1600\n",
      "Iteration 138, loss = 0.26080044\n",
      "Training epoch 139/1600\n",
      "Iteration 139, loss = 0.26053581\n",
      "Training epoch 140/1600\n",
      "Iteration 140, loss = 0.26027902\n",
      "Training epoch 141/1600\n",
      "Iteration 141, loss = 0.25999878\n",
      "Training epoch 142/1600\n",
      "Iteration 142, loss = 0.25974151\n",
      "Training epoch 143/1600\n",
      "Iteration 143, loss = 0.25949275\n",
      "Training epoch 144/1600\n",
      "Iteration 144, loss = 0.25920956\n",
      "Training epoch 145/1600\n",
      "Iteration 145, loss = 0.25896303\n",
      "Training epoch 146/1600\n",
      "Iteration 146, loss = 0.25870876\n",
      "Training epoch 147/1600\n",
      "Iteration 147, loss = 0.25847705\n",
      "Training epoch 148/1600\n",
      "Iteration 148, loss = 0.25824282\n",
      "Training epoch 149/1600\n",
      "Iteration 149, loss = 0.25800237\n",
      "Training epoch 150/1600\n",
      "Iteration 150, loss = 0.25779090\n",
      "Training epoch 151/1600\n",
      "Iteration 151, loss = 0.25755628\n",
      "Training epoch 152/1600\n",
      "Iteration 152, loss = 0.25732529\n",
      "Training epoch 153/1600\n",
      "Iteration 153, loss = 0.25709667\n",
      "Training epoch 154/1600\n",
      "Iteration 154, loss = 0.25687538\n",
      "Training epoch 155/1600\n",
      "Iteration 155, loss = 0.25664158\n",
      "Training epoch 156/1600\n",
      "Iteration 156, loss = 0.25644417\n",
      "Training epoch 157/1600\n",
      "Iteration 157, loss = 0.25623438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 158/1600\n",
      "Iteration 158, loss = 0.25601090\n",
      "Training epoch 159/1600\n",
      "Iteration 159, loss = 0.25581747\n",
      "Training epoch 160/1600\n",
      "Iteration 160, loss = 0.25560869\n",
      "Training epoch 161/1600\n",
      "Iteration 161, loss = 0.25543029\n",
      "Training epoch 162/1600\n",
      "Iteration 162, loss = 0.25526704\n",
      "Training epoch 163/1600\n",
      "Iteration 163, loss = 0.25510873\n",
      "Training epoch 164/1600\n",
      "Iteration 164, loss = 0.25498688\n",
      "Training epoch 165/1600\n",
      "Iteration 165, loss = 0.25481962\n",
      "Training epoch 166/1600\n",
      "Iteration 166, loss = 0.25470110\n",
      "Training epoch 167/1600\n",
      "Iteration 167, loss = 0.25457403\n",
      "Training epoch 168/1600\n",
      "Iteration 168, loss = 0.25454638\n",
      "Training epoch 169/1600\n",
      "Iteration 169, loss = 0.25451705\n",
      "Training epoch 170/1600\n",
      "Iteration 170, loss = 0.25450717\n",
      "Training epoch 171/1600\n",
      "Iteration 171, loss = 0.25441708\n",
      "Training epoch 172/1600\n",
      "Iteration 172, loss = 0.25446389\n",
      "Training epoch 173/1600\n",
      "Iteration 173, loss = 0.25437466\n",
      "Training epoch 174/1600\n",
      "Iteration 174, loss = 0.25457590\n",
      "Training epoch 175/1600\n",
      "Iteration 175, loss = 0.25480409\n",
      "Training epoch 176/1600\n",
      "Iteration 176, loss = 0.25548109\n",
      "Training epoch 177/1600\n",
      "Iteration 177, loss = 0.25558696\n",
      "Training epoch 178/1600\n",
      "Iteration 178, loss = 0.25652687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 179/1600\n",
      "Iteration 179, loss = 0.25660810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 180/1600\n",
      "Iteration 180, loss = 0.25801025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 181/1600\n",
      "Iteration 181, loss = 0.25843939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 182/1600\n",
      "Iteration 182, loss = 0.26098439\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 183/1600\n",
      "Iteration 183, loss = 0.26073898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 184/1600\n",
      "Iteration 184, loss = 0.26483065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 185/1600\n",
      "Iteration 185, loss = 0.26298200\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 186/1600\n",
      "Iteration 186, loss = 0.26701914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 187/1600\n",
      "Iteration 187, loss = 0.26396704\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 188/1600\n",
      "Iteration 188, loss = 0.26813375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 189/1600\n",
      "Iteration 189, loss = 0.26389623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 190/1600\n",
      "Iteration 190, loss = 0.26779631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 191/1600\n",
      "Iteration 191, loss = 0.26380876\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 192/1600\n",
      "Iteration 192, loss = 0.26730540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 193/1600\n",
      "Iteration 193, loss = 0.26256462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 194/1600\n",
      "Iteration 194, loss = 0.26502191\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 195/1600\n",
      "Iteration 195, loss = 0.26129545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 196/1600\n",
      "Iteration 196, loss = 0.26350510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 197/1600\n",
      "Iteration 197, loss = 0.26031858\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 198/1600\n",
      "Iteration 198, loss = 0.26228446\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 199/1600\n",
      "Iteration 199, loss = 0.25947998\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 200/1600\n",
      "Iteration 200, loss = 0.26115616\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 201/1600\n",
      "Iteration 201, loss = 0.25845330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 202/1600\n",
      "Iteration 202, loss = 0.25982217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 203/1600\n",
      "Iteration 203, loss = 0.25763433\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 204/1600\n",
      "Iteration 204, loss = 0.25880795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 205/1600\n",
      "Iteration 205, loss = 0.25683472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 206/1600\n",
      "Iteration 206, loss = 0.25776201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 207/1600\n",
      "Iteration 207, loss = 0.25607667\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 208/1600\n",
      "Iteration 208, loss = 0.25694802\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 209/1600\n",
      "Iteration 209, loss = 0.25537857\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 210/1600\n",
      "Iteration 210, loss = 0.25614675\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 211/1600\n",
      "Iteration 211, loss = 0.25479496\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 212/1600\n",
      "Iteration 212, loss = 0.25537523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 213/1600\n",
      "Iteration 213, loss = 0.25433460\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 214/1600\n",
      "Iteration 214, loss = 0.25503798\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 215/1600\n",
      "Iteration 215, loss = 0.25405005\n",
      "Training epoch 216/1600\n",
      "Iteration 216, loss = 0.25474542\n",
      "Training epoch 217/1600\n",
      "Iteration 217, loss = 0.25380368\n",
      "Training epoch 218/1600\n",
      "Iteration 218, loss = 0.25449174\n",
      "Training epoch 219/1600\n",
      "Iteration 219, loss = 0.25363096\n",
      "Training epoch 220/1600\n",
      "Iteration 220, loss = 0.25442150\n",
      "Training epoch 221/1600\n",
      "Iteration 221, loss = 0.25347478\n",
      "Training epoch 222/1600\n",
      "Iteration 222, loss = 0.25430748\n",
      "Training epoch 223/1600\n",
      "Iteration 223, loss = 0.25338811\n",
      "Training epoch 224/1600\n",
      "Iteration 224, loss = 0.25425978\n",
      "Training epoch 225/1600\n",
      "Iteration 225, loss = 0.25347321\n",
      "Training epoch 226/1600\n",
      "Iteration 226, loss = 0.25447624\n",
      "Training epoch 227/1600\n",
      "Iteration 227, loss = 0.25348957\n",
      "Training epoch 228/1600\n",
      "Iteration 228, loss = 0.25452306\n",
      "Training epoch 229/1600\n",
      "Iteration 229, loss = 0.25357628\n",
      "Training epoch 230/1600\n",
      "Iteration 230, loss = 0.25465387\n",
      "Training epoch 231/1600\n",
      "Iteration 231, loss = 0.25366773\n",
      "Training epoch 232/1600\n",
      "Iteration 232, loss = 0.25490726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 233/1600\n",
      "Iteration 233, loss = 0.25403349\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 234/1600\n",
      "Iteration 234, loss = 0.25548706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 235/1600\n",
      "Iteration 235, loss = 0.25442953\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 236/1600\n",
      "Iteration 236, loss = 0.25599053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 237/1600\n",
      "Iteration 237, loss = 0.25454112\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 238/1600\n",
      "Iteration 238, loss = 0.25605388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 239/1600\n",
      "Iteration 239, loss = 0.25447734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 240/1600\n",
      "Iteration 240, loss = 0.25580895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 241/1600\n",
      "Iteration 241, loss = 0.25416276\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 242/1600\n",
      "Iteration 242, loss = 0.25565695\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 243/1600\n",
      "Iteration 243, loss = 0.25428445\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 244/1600\n",
      "Iteration 244, loss = 0.25582767\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 245/1600\n",
      "Iteration 245, loss = 0.25409337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 246/1600\n",
      "Iteration 246, loss = 0.25532791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 247/1600\n",
      "Iteration 247, loss = 0.25374312\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 248/1600\n",
      "Iteration 248, loss = 0.25490817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 249/1600\n",
      "Iteration 249, loss = 0.25334425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 250/1600\n",
      "Iteration 250, loss = 0.25442151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 251/1600\n",
      "Iteration 251, loss = 0.25309342\n",
      "Training epoch 252/1600\n",
      "Iteration 252, loss = 0.25434687\n",
      "Training epoch 253/1600\n",
      "Iteration 253, loss = 0.25298346\n",
      "Training epoch 254/1600\n",
      "Iteration 254, loss = 0.25417979\n",
      "Training epoch 255/1600\n",
      "Iteration 255, loss = 0.25287858\n",
      "Training epoch 256/1600\n",
      "Iteration 256, loss = 0.25411049\n",
      "Training epoch 257/1600\n",
      "Iteration 257, loss = 0.25280034\n",
      "Training epoch 258/1600\n",
      "Iteration 258, loss = 0.25407558\n",
      "Training epoch 259/1600\n",
      "Iteration 259, loss = 0.25287141\n",
      "Training epoch 260/1600\n",
      "Iteration 260, loss = 0.25393459\n",
      "Training epoch 261/1600\n",
      "Iteration 261, loss = 0.25259832\n",
      "Training epoch 262/1600\n",
      "Iteration 262, loss = 0.25383457\n",
      "Training epoch 263/1600\n",
      "Iteration 263, loss = 0.25244158\n",
      "Training epoch 264/1600\n",
      "Iteration 264, loss = 0.25339944\n",
      "Training epoch 265/1600\n",
      "Iteration 265, loss = 0.25210025\n",
      "Training epoch 266/1600\n",
      "Iteration 266, loss = 0.25296678\n",
      "Training epoch 267/1600\n",
      "Iteration 267, loss = 0.25176423\n",
      "Training epoch 268/1600\n",
      "Iteration 268, loss = 0.25261619\n",
      "Training epoch 269/1600\n",
      "Iteration 269, loss = 0.25152442\n",
      "Training epoch 270/1600\n",
      "Iteration 270, loss = 0.25246196\n",
      "Training epoch 271/1600\n",
      "Iteration 271, loss = 0.25148995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 272/1600\n",
      "Iteration 272, loss = 0.25249340\n",
      "Training epoch 273/1600\n",
      "Iteration 273, loss = 0.25173992\n",
      "Training epoch 274/1600\n",
      "Iteration 274, loss = 0.25284113\n",
      "Training epoch 275/1600\n",
      "Iteration 275, loss = 0.25175034\n",
      "Training epoch 276/1600\n",
      "Iteration 276, loss = 0.25287090\n",
      "Training epoch 277/1600\n",
      "Iteration 277, loss = 0.25177448\n",
      "Training epoch 278/1600\n",
      "Iteration 278, loss = 0.25292798\n",
      "Training epoch 279/1600\n",
      "Iteration 279, loss = 0.25167613\n",
      "Training epoch 280/1600\n",
      "Iteration 280, loss = 0.25263204\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 281/1600\n",
      "Iteration 281, loss = 0.25142209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 282/1600\n",
      "Iteration 282, loss = 0.25233334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 283/1600\n",
      "Iteration 283, loss = 0.25118308\n",
      "Training epoch 284/1600\n",
      "Iteration 284, loss = 0.25190531\n",
      "Training epoch 285/1600\n",
      "Iteration 285, loss = 0.25088671\n",
      "Training epoch 286/1600\n",
      "Iteration 286, loss = 0.25179311\n",
      "Training epoch 287/1600\n",
      "Iteration 287, loss = 0.25085226\n",
      "Training epoch 288/1600\n",
      "Iteration 288, loss = 0.25170650\n",
      "Training epoch 289/1600\n",
      "Iteration 289, loss = 0.25073236\n",
      "Training epoch 290/1600\n",
      "Iteration 290, loss = 0.25171250\n",
      "Training epoch 291/1600\n",
      "Iteration 291, loss = 0.25082500\n",
      "Training epoch 292/1600\n",
      "Iteration 292, loss = 0.25174209\n",
      "Training epoch 293/1600\n",
      "Iteration 293, loss = 0.25080649\n",
      "Training epoch 294/1600\n",
      "Iteration 294, loss = 0.25172963\n",
      "Training epoch 295/1600\n",
      "Iteration 295, loss = 0.25071403\n",
      "Training epoch 296/1600\n",
      "Iteration 296, loss = 0.25162081\n",
      "Training epoch 297/1600\n",
      "Iteration 297, loss = 0.25070243\n",
      "Training epoch 298/1600\n",
      "Iteration 298, loss = 0.25149119\n",
      "Training epoch 299/1600\n",
      "Iteration 299, loss = 0.25042668\n",
      "Training epoch 300/1600\n",
      "Iteration 300, loss = 0.25116185\n",
      "Training epoch 301/1600\n",
      "Iteration 301, loss = 0.25021272\n",
      "Training epoch 302/1600\n",
      "Iteration 302, loss = 0.25095204\n",
      "Training epoch 303/1600\n",
      "Iteration 303, loss = 0.25011776\n",
      "Training epoch 304/1600\n",
      "Iteration 304, loss = 0.25089390\n",
      "Training epoch 305/1600\n",
      "Iteration 305, loss = 0.25013589\n",
      "Training epoch 306/1600\n",
      "Iteration 306, loss = 0.25104549\n",
      "Training epoch 307/1600\n",
      "Iteration 307, loss = 0.25020036\n",
      "Training epoch 308/1600\n",
      "Iteration 308, loss = 0.25117973\n",
      "Training epoch 309/1600\n",
      "Iteration 309, loss = 0.25032278\n",
      "Training epoch 310/1600\n",
      "Iteration 310, loss = 0.25134082\n",
      "Training epoch 311/1600\n",
      "Iteration 311, loss = 0.25048148\n",
      "Training epoch 312/1600\n",
      "Iteration 312, loss = 0.25155311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 313/1600\n",
      "Iteration 313, loss = 0.25049553\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 314/1600\n",
      "Iteration 314, loss = 0.25144213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 315/1600\n",
      "Iteration 315, loss = 0.25046243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 316/1600\n",
      "Iteration 316, loss = 0.25150740\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 317/1600\n",
      "Iteration 317, loss = 0.25056303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 318/1600\n",
      "Iteration 318, loss = 0.25167042\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 319/1600\n",
      "Iteration 319, loss = 0.25052032\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 320/1600\n",
      "Iteration 320, loss = 0.25144476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 321/1600\n",
      "Iteration 321, loss = 0.25025772\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 322/1600\n",
      "Iteration 322, loss = 0.25103776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 323/1600\n",
      "Iteration 323, loss = 0.25006146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 324/1600\n",
      "Iteration 324, loss = 0.25081154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 325/1600\n",
      "Iteration 325, loss = 0.24977596\n",
      "Training epoch 326/1600\n",
      "Iteration 326, loss = 0.25051337\n",
      "Training epoch 327/1600\n",
      "Iteration 327, loss = 0.24961180\n",
      "Training epoch 328/1600\n",
      "Iteration 328, loss = 0.25029526\n",
      "Training epoch 329/1600\n",
      "Iteration 329, loss = 0.24949490\n",
      "Training epoch 330/1600\n",
      "Iteration 330, loss = 0.25028995\n",
      "Training epoch 331/1600\n",
      "Iteration 331, loss = 0.24944074\n",
      "Training epoch 332/1600\n",
      "Iteration 332, loss = 0.25027833\n",
      "Training epoch 333/1600\n",
      "Iteration 333, loss = 0.24942618\n",
      "Training epoch 334/1600\n",
      "Iteration 334, loss = 0.25015041\n",
      "Training epoch 335/1600\n",
      "Iteration 335, loss = 0.24934015\n",
      "Training epoch 336/1600\n",
      "Iteration 336, loss = 0.25007003\n",
      "Training epoch 337/1600\n",
      "Iteration 337, loss = 0.24928518\n",
      "Training epoch 338/1600\n",
      "Iteration 338, loss = 0.25004065\n",
      "Training epoch 339/1600\n",
      "Iteration 339, loss = 0.24921586\n",
      "Training epoch 340/1600\n",
      "Iteration 340, loss = 0.24997808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 341/1600\n",
      "Iteration 341, loss = 0.24920897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 342/1600\n",
      "Iteration 342, loss = 0.24978089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 343/1600\n",
      "Iteration 343, loss = 0.24888451\n",
      "Training epoch 344/1600\n",
      "Iteration 344, loss = 0.24922097\n",
      "Training epoch 345/1600\n",
      "Iteration 345, loss = 0.24830574\n",
      "Training epoch 346/1600\n",
      "Iteration 346, loss = 0.24846840\n",
      "Training epoch 347/1600\n",
      "Iteration 347, loss = 0.24779198\n",
      "Training epoch 348/1600\n",
      "Iteration 348, loss = 0.24793888\n",
      "Training epoch 349/1600\n",
      "Iteration 349, loss = 0.24744075\n",
      "Training epoch 350/1600\n",
      "Iteration 350, loss = 0.24764529\n",
      "Training epoch 351/1600\n",
      "Iteration 351, loss = 0.24723142\n",
      "Training epoch 352/1600\n",
      "Iteration 352, loss = 0.24744203\n",
      "Training epoch 353/1600\n",
      "Iteration 353, loss = 0.24710733\n",
      "Training epoch 354/1600\n",
      "Iteration 354, loss = 0.24741581\n",
      "Training epoch 355/1600\n",
      "Iteration 355, loss = 0.24710121\n",
      "Training epoch 356/1600\n",
      "Iteration 356, loss = 0.24742664\n",
      "Training epoch 357/1600\n",
      "Iteration 357, loss = 0.24712205\n",
      "Training epoch 358/1600\n",
      "Iteration 358, loss = 0.24752184\n",
      "Training epoch 359/1600\n",
      "Iteration 359, loss = 0.24721719\n",
      "Training epoch 360/1600\n",
      "Iteration 360, loss = 0.24763900\n",
      "Training epoch 361/1600\n",
      "Iteration 361, loss = 0.24735508\n",
      "Training epoch 362/1600\n",
      "Iteration 362, loss = 0.24785447\n",
      "Training epoch 363/1600\n",
      "Iteration 363, loss = 0.24756498\n",
      "Training epoch 364/1600\n",
      "Iteration 364, loss = 0.24809959\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 365/1600\n",
      "Iteration 365, loss = 0.24774632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 366/1600\n",
      "Iteration 366, loss = 0.24833534\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 367/1600\n",
      "Iteration 367, loss = 0.24790977\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 368/1600\n",
      "Iteration 368, loss = 0.24854260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 369/1600\n",
      "Iteration 369, loss = 0.24806991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 370/1600\n",
      "Iteration 370, loss = 0.24876641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 371/1600\n",
      "Iteration 371, loss = 0.24823769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 372/1600\n",
      "Iteration 372, loss = 0.24898896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 373/1600\n",
      "Iteration 373, loss = 0.24848222\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 374/1600\n",
      "Iteration 374, loss = 0.24929997\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 375/1600\n",
      "Iteration 375, loss = 0.24876043\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 376/1600\n",
      "Iteration 376, loss = 0.24964109\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 377/1600\n",
      "Iteration 377, loss = 0.24893196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 378/1600\n",
      "Iteration 378, loss = 0.24988330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 379/1600\n",
      "Iteration 379, loss = 0.24913934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 380/1600\n",
      "Iteration 380, loss = 0.25011640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 381/1600\n",
      "Iteration 381, loss = 0.24916474\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 382/1600\n",
      "Iteration 382, loss = 0.25009623\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 383/1600\n",
      "Iteration 383, loss = 0.24912458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 384/1600\n",
      "Iteration 384, loss = 0.24995734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 385/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 385, loss = 0.24901612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 386/1600\n",
      "Iteration 386, loss = 0.24987978\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 387/1600\n",
      "Iteration 387, loss = 0.24895340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 388/1600\n",
      "Iteration 388, loss = 0.24974914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 389/1600\n",
      "Iteration 389, loss = 0.24882360\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 390/1600\n",
      "Iteration 390, loss = 0.24960345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 391/1600\n",
      "Iteration 391, loss = 0.24869591\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 392/1600\n",
      "Iteration 392, loss = 0.24937470\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 393/1600\n",
      "Iteration 393, loss = 0.24851845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 394/1600\n",
      "Iteration 394, loss = 0.24917829\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 395/1600\n",
      "Iteration 395, loss = 0.24836396\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 396/1600\n",
      "Iteration 396, loss = 0.24889440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 397/1600\n",
      "Iteration 397, loss = 0.24814648\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 398/1600\n",
      "Iteration 398, loss = 0.24873239\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 399/1600\n",
      "Iteration 399, loss = 0.24798714\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 400/1600\n",
      "Iteration 400, loss = 0.24852995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 401/1600\n",
      "Iteration 401, loss = 0.24792569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 402/1600\n",
      "Iteration 402, loss = 0.24851811\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 403/1600\n",
      "Iteration 403, loss = 0.24784138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 404/1600\n",
      "Iteration 404, loss = 0.24841665\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 405/1600\n",
      "Iteration 405, loss = 0.24780785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 406/1600\n",
      "Iteration 406, loss = 0.24836465\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 407/1600\n",
      "Iteration 407, loss = 0.24771812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 408/1600\n",
      "Iteration 408, loss = 0.24827480\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 409/1600\n",
      "Iteration 409, loss = 0.24762666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 410/1600\n",
      "Iteration 410, loss = 0.24804758\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 411/1600\n",
      "Iteration 411, loss = 0.24745156\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 412/1600\n",
      "Iteration 412, loss = 0.24793781\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 413/1600\n",
      "Iteration 413, loss = 0.24738096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 414/1600\n",
      "Iteration 414, loss = 0.24783167\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 415/1600\n",
      "Iteration 415, loss = 0.24729992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 416/1600\n",
      "Iteration 416, loss = 0.24777954\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 417/1600\n",
      "Iteration 417, loss = 0.24725689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 418/1600\n",
      "Iteration 418, loss = 0.24771324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 419/1600\n",
      "Iteration 419, loss = 0.24723062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 420/1600\n",
      "Iteration 420, loss = 0.24771124\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 421/1600\n",
      "Iteration 421, loss = 0.24721644\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 422/1600\n",
      "Iteration 422, loss = 0.24769370\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 423/1600\n",
      "Iteration 423, loss = 0.24720822\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 424/1600\n",
      "Iteration 424, loss = 0.24769763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 425/1600\n",
      "Iteration 425, loss = 0.24719799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 426/1600\n",
      "Iteration 426, loss = 0.24771676\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 427/1600\n",
      "Iteration 427, loss = 0.24722346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 428/1600\n",
      "Iteration 428, loss = 0.24770357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 429/1600\n",
      "Iteration 429, loss = 0.24719422\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 430/1600\n",
      "Iteration 430, loss = 0.24767996\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 431/1600\n",
      "Iteration 431, loss = 0.24716987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 432/1600\n",
      "Iteration 432, loss = 0.24766347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 433/1600\n",
      "Iteration 433, loss = 0.24714706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 434/1600\n",
      "Iteration 434, loss = 0.24756357\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 435/1600\n",
      "Iteration 435, loss = 0.24708233\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 436/1600\n",
      "Iteration 436, loss = 0.24757216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 437/1600\n",
      "Iteration 437, loss = 0.24724153\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 438/1600\n",
      "Iteration 438, loss = 0.24784633\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 439/1600\n",
      "Iteration 439, loss = 0.24734856\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 440/1600\n",
      "Iteration 440, loss = 0.24792886\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 441/1600\n",
      "Iteration 441, loss = 0.24739511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 442/1600\n",
      "Iteration 442, loss = 0.24797511\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 443/1600\n",
      "Iteration 443, loss = 0.24737729\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 444/1600\n",
      "Iteration 444, loss = 0.24793469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 445/1600\n",
      "Iteration 445, loss = 0.24732844\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 446/1600\n",
      "Iteration 446, loss = 0.24789347\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 447/1600\n",
      "Iteration 447, loss = 0.24733472\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 448/1600\n",
      "Iteration 448, loss = 0.24790229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 449/1600\n",
      "Iteration 449, loss = 0.24731271\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 450/1600\n",
      "Iteration 450, loss = 0.24793749\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 451/1600\n",
      "Iteration 451, loss = 0.24740087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 452/1600\n",
      "Iteration 452, loss = 0.24796971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 453/1600\n",
      "Iteration 453, loss = 0.24731940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 454/1600\n",
      "Iteration 454, loss = 0.24783725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 455/1600\n",
      "Iteration 455, loss = 0.24722319\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 456/1600\n",
      "Iteration 456, loss = 0.24771049\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 457/1600\n",
      "Iteration 457, loss = 0.24713336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 458/1600\n",
      "Iteration 458, loss = 0.24755786\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 459/1600\n",
      "Iteration 459, loss = 0.24703395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 460/1600\n",
      "Iteration 460, loss = 0.24748253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 461/1600\n",
      "Iteration 461, loss = 0.24697638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 462/1600\n",
      "Iteration 462, loss = 0.24743589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 463/1600\n",
      "Iteration 463, loss = 0.24691711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 464/1600\n",
      "Iteration 464, loss = 0.24733311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 465/1600\n",
      "Iteration 465, loss = 0.24685917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 466/1600\n",
      "Iteration 466, loss = 0.24733307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 467/1600\n",
      "Iteration 467, loss = 0.24685476\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 468/1600\n",
      "Iteration 468, loss = 0.24732933\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 469/1600\n",
      "Iteration 469, loss = 0.24684841\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 470/1600\n",
      "Iteration 470, loss = 0.24730249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 471/1600\n",
      "Iteration 471, loss = 0.24679597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 472/1600\n",
      "Iteration 472, loss = 0.24726691\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 473/1600\n",
      "Iteration 473, loss = 0.24680462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 474/1600\n",
      "Iteration 474, loss = 0.24726652\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 475/1600\n",
      "Iteration 475, loss = 0.24679414\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 476/1600\n",
      "Iteration 476, loss = 0.24722458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 477/1600\n",
      "Iteration 477, loss = 0.24671526\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 478/1600\n",
      "Iteration 478, loss = 0.24713361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 479/1600\n",
      "Iteration 479, loss = 0.24667934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 480/1600\n",
      "Iteration 480, loss = 0.24711163\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 481/1600\n",
      "Iteration 481, loss = 0.24664066\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 482/1600\n",
      "Iteration 482, loss = 0.24706388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 483/1600\n",
      "Iteration 483, loss = 0.24662539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 484/1600\n",
      "Iteration 484, loss = 0.24702282\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 485/1600\n",
      "Iteration 485, loss = 0.24656390\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 486/1600\n",
      "Iteration 486, loss = 0.24693429\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 487/1600\n",
      "Iteration 487, loss = 0.24652853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 488/1600\n",
      "Iteration 488, loss = 0.24696824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 489/1600\n",
      "Iteration 489, loss = 0.24654309\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 490/1600\n",
      "Iteration 490, loss = 0.24698716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 491/1600\n",
      "Iteration 491, loss = 0.24664817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 492/1600\n",
      "Iteration 492, loss = 0.24715419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 493/1600\n",
      "Iteration 493, loss = 0.24668616\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 494/1600\n",
      "Iteration 494, loss = 0.24714615\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 495/1600\n",
      "Iteration 495, loss = 0.24670162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 496/1600\n",
      "Iteration 496, loss = 0.24718587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 497/1600\n",
      "Iteration 497, loss = 0.24667462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 498/1600\n",
      "Iteration 498, loss = 0.24710728\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 499/1600\n",
      "Iteration 499, loss = 0.24661716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 500/1600\n",
      "Iteration 500, loss = 0.24706434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 501/1600\n",
      "Iteration 501, loss = 0.24659190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 502/1600\n",
      "Iteration 502, loss = 0.24697107\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 503/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 503, loss = 0.24651464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 504/1600\n",
      "Iteration 504, loss = 0.24690154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 505/1600\n",
      "Iteration 505, loss = 0.24646102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 506/1600\n",
      "Iteration 506, loss = 0.24687226\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 507/1600\n",
      "Iteration 507, loss = 0.24645018\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 508/1600\n",
      "Iteration 508, loss = 0.24688393\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 509/1600\n",
      "Iteration 509, loss = 0.24642973\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 510/1600\n",
      "Iteration 510, loss = 0.24680967\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 511/1600\n",
      "Iteration 511, loss = 0.24640361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 512/1600\n",
      "Iteration 512, loss = 0.24681080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 513/1600\n",
      "Iteration 513, loss = 0.24638456\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 514/1600\n",
      "Iteration 514, loss = 0.24679125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 515/1600\n",
      "Iteration 515, loss = 0.24639437\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 516/1600\n",
      "Iteration 516, loss = 0.24682313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 517/1600\n",
      "Iteration 517, loss = 0.24644545\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 518/1600\n",
      "Iteration 518, loss = 0.24688064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 519/1600\n",
      "Iteration 519, loss = 0.24641608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 520/1600\n",
      "Iteration 520, loss = 0.24694601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 521/1600\n",
      "Iteration 521, loss = 0.24649912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 522/1600\n",
      "Iteration 522, loss = 0.24695025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 523/1600\n",
      "Iteration 523, loss = 0.24645989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 524/1600\n",
      "Iteration 524, loss = 0.24686702\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 525/1600\n",
      "Iteration 525, loss = 0.24640502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 526/1600\n",
      "Iteration 526, loss = 0.24679379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 527/1600\n",
      "Iteration 527, loss = 0.24635782\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 528/1600\n",
      "Iteration 528, loss = 0.24670096\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 529/1600\n",
      "Iteration 529, loss = 0.24629540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 530/1600\n",
      "Iteration 530, loss = 0.24668601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 531/1600\n",
      "Iteration 531, loss = 0.24625490\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 532/1600\n",
      "Iteration 532, loss = 0.24662619\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 533/1600\n",
      "Iteration 533, loss = 0.24620776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 534/1600\n",
      "Iteration 534, loss = 0.24657991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 535/1600\n",
      "Iteration 535, loss = 0.24616444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 536/1600\n",
      "Iteration 536, loss = 0.24654554\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 537/1600\n",
      "Iteration 537, loss = 0.24616249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 538/1600\n",
      "Iteration 538, loss = 0.24654453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 539/1600\n",
      "Iteration 539, loss = 0.24614958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 540/1600\n",
      "Iteration 540, loss = 0.24655528\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 541/1600\n",
      "Iteration 541, loss = 0.24616859\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 542/1600\n",
      "Iteration 542, loss = 0.24657165\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 543/1600\n",
      "Iteration 543, loss = 0.24626088\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 544/1600\n",
      "Iteration 544, loss = 0.24668996\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 545/1600\n",
      "Iteration 545, loss = 0.24629506\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 546/1600\n",
      "Iteration 546, loss = 0.24672900\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 547/1600\n",
      "Iteration 547, loss = 0.24628105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 548/1600\n",
      "Iteration 548, loss = 0.24669236\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 549/1600\n",
      "Iteration 549, loss = 0.24623946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 550/1600\n",
      "Iteration 550, loss = 0.24661847\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 551/1600\n",
      "Iteration 551, loss = 0.24616544\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 552/1600\n",
      "Iteration 552, loss = 0.24651712\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 553/1600\n",
      "Iteration 553, loss = 0.24611260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 554/1600\n",
      "Iteration 554, loss = 0.24643180\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 555/1600\n",
      "Iteration 555, loss = 0.24602385\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 556/1600\n",
      "Iteration 556, loss = 0.24636782\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 557/1600\n",
      "Iteration 557, loss = 0.24597033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 558/1600\n",
      "Iteration 558, loss = 0.24630979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 559/1600\n",
      "Iteration 559, loss = 0.24595266\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 560/1600\n",
      "Iteration 560, loss = 0.24629575\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 561/1600\n",
      "Iteration 561, loss = 0.24592781\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 562/1600\n",
      "Iteration 562, loss = 0.24627330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 563/1600\n",
      "Iteration 563, loss = 0.24593131\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 564/1600\n",
      "Iteration 564, loss = 0.24629217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 565/1600\n",
      "Iteration 565, loss = 0.24592260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 566/1600\n",
      "Iteration 566, loss = 0.24628640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 567/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 567, loss = 0.24591339\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 568/1600\n",
      "Iteration 568, loss = 0.24627019\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 569/1600\n",
      "Iteration 569, loss = 0.24589964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 570/1600\n",
      "Iteration 570, loss = 0.24623594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 571/1600\n",
      "Iteration 571, loss = 0.24590498\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 572/1600\n",
      "Iteration 572, loss = 0.24628095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 573/1600\n",
      "Iteration 573, loss = 0.24595344\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 574/1600\n",
      "Iteration 574, loss = 0.24630868\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 575/1600\n",
      "Iteration 575, loss = 0.24592214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 576/1600\n",
      "Iteration 576, loss = 0.24627068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 577/1600\n",
      "Iteration 577, loss = 0.24588506\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 578/1600\n",
      "Iteration 578, loss = 0.24619196\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 579/1600\n",
      "Iteration 579, loss = 0.24583301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 580/1600\n",
      "Iteration 580, loss = 0.24615661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 581/1600\n",
      "Iteration 581, loss = 0.24580459\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 582/1600\n",
      "Iteration 582, loss = 0.24609834\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 583/1600\n",
      "Iteration 583, loss = 0.24578581\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 584/1600\n",
      "Iteration 584, loss = 0.24614669\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 585/1600\n",
      "Iteration 585, loss = 0.24580914\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 586/1600\n",
      "Iteration 586, loss = 0.24617837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 587/1600\n",
      "Iteration 587, loss = 0.24582551\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 588/1600\n",
      "Iteration 588, loss = 0.24619318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 589/1600\n",
      "Iteration 589, loss = 0.24583802\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 590/1600\n",
      "Iteration 590, loss = 0.24619917\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 591/1600\n",
      "Iteration 591, loss = 0.24585760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 592/1600\n",
      "Iteration 592, loss = 0.24621219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 593/1600\n",
      "Iteration 593, loss = 0.24584898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 594/1600\n",
      "Iteration 594, loss = 0.24619582\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 595/1600\n",
      "Iteration 595, loss = 0.24583510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 596/1600\n",
      "Iteration 596, loss = 0.24618463\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 597/1600\n",
      "Iteration 597, loss = 0.24583870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 598/1600\n",
      "Iteration 598, loss = 0.24620525\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 599/1600\n",
      "Iteration 599, loss = 0.24583554\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 600/1600\n",
      "Iteration 600, loss = 0.24615229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 601/1600\n",
      "Iteration 601, loss = 0.24577966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 602/1600\n",
      "Iteration 602, loss = 0.24612431\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 603/1600\n",
      "Iteration 603, loss = 0.24577173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 604/1600\n",
      "Iteration 604, loss = 0.24612314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 605/1600\n",
      "Iteration 605, loss = 0.24576894\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 606/1600\n",
      "Iteration 606, loss = 0.24610820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 607/1600\n",
      "Iteration 607, loss = 0.24576944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 608/1600\n",
      "Iteration 608, loss = 0.24612721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 609/1600\n",
      "Iteration 609, loss = 0.24581253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 610/1600\n",
      "Iteration 610, loss = 0.24619036\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 611/1600\n",
      "Iteration 611, loss = 0.24584095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 612/1600\n",
      "Iteration 612, loss = 0.24620404\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 613/1600\n",
      "Iteration 613, loss = 0.24581202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 614/1600\n",
      "Iteration 614, loss = 0.24615497\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 615/1600\n",
      "Iteration 615, loss = 0.24578257\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 616/1600\n",
      "Iteration 616, loss = 0.24619027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 617/1600\n",
      "Iteration 617, loss = 0.24580878\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 618/1600\n",
      "Iteration 618, loss = 0.24614425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 619/1600\n",
      "Iteration 619, loss = 0.24577031\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 620/1600\n",
      "Iteration 620, loss = 0.24610268\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 621/1600\n",
      "Iteration 621, loss = 0.24572469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 622/1600\n",
      "Iteration 622, loss = 0.24600819\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 623/1600\n",
      "Iteration 623, loss = 0.24565383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 624/1600\n",
      "Iteration 624, loss = 0.24595396\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 625/1600\n",
      "Iteration 625, loss = 0.24559775\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 626/1600\n",
      "Iteration 626, loss = 0.24587763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 627/1600\n",
      "Iteration 627, loss = 0.24553583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 628/1600\n",
      "Iteration 628, loss = 0.24582205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 629/1600\n",
      "Iteration 629, loss = 0.24549037\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 630/1600\n",
      "Iteration 630, loss = 0.24574989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 631/1600\n",
      "Iteration 631, loss = 0.24546329\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 632/1600\n",
      "Iteration 632, loss = 0.24573980\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 633/1600\n",
      "Iteration 633, loss = 0.24542056\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 634/1600\n",
      "Iteration 634, loss = 0.24567776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 635/1600\n",
      "Iteration 635, loss = 0.24537592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 636/1600\n",
      "Iteration 636, loss = 0.24565214\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 637/1600\n",
      "Iteration 637, loss = 0.24535254\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 638/1600\n",
      "Iteration 638, loss = 0.24563129\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 639/1600\n",
      "Iteration 639, loss = 0.24535324\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 640/1600\n",
      "Iteration 640, loss = 0.24563606\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 641/1600\n",
      "Iteration 641, loss = 0.24538116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 642/1600\n",
      "Iteration 642, loss = 0.24566694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 643/1600\n",
      "Iteration 643, loss = 0.24536997\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 644/1600\n",
      "Iteration 644, loss = 0.24563994\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 645/1600\n",
      "Iteration 645, loss = 0.24535355\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 646/1600\n",
      "Iteration 646, loss = 0.24564343\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 647/1600\n",
      "Iteration 647, loss = 0.24537865\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 648/1600\n",
      "Iteration 648, loss = 0.24568948\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 649/1600\n",
      "Iteration 649, loss = 0.24539246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 650/1600\n",
      "Iteration 650, loss = 0.24569455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 651/1600\n",
      "Iteration 651, loss = 0.24539251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 652/1600\n",
      "Iteration 652, loss = 0.24567405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 653/1600\n",
      "Iteration 653, loss = 0.24538432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 654/1600\n",
      "Iteration 654, loss = 0.24568532\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 655/1600\n",
      "Iteration 655, loss = 0.24542549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 656/1600\n",
      "Iteration 656, loss = 0.24574005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 657/1600\n",
      "Iteration 657, loss = 0.24544717\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 658/1600\n",
      "Iteration 658, loss = 0.24575718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 659/1600\n",
      "Iteration 659, loss = 0.24549020\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 660/1600\n",
      "Iteration 660, loss = 0.24580434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 661/1600\n",
      "Iteration 661, loss = 0.24547169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 662/1600\n",
      "Iteration 662, loss = 0.24578072\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 663/1600\n",
      "Iteration 663, loss = 0.24544389\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 664/1600\n",
      "Iteration 664, loss = 0.24576094\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 665/1600\n",
      "Iteration 665, loss = 0.24543137\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 666/1600\n",
      "Iteration 666, loss = 0.24573688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 667/1600\n",
      "Iteration 667, loss = 0.24542677\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 668/1600\n",
      "Iteration 668, loss = 0.24573201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 669/1600\n",
      "Iteration 669, loss = 0.24543375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 670/1600\n",
      "Iteration 670, loss = 0.24572058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 671/1600\n",
      "Iteration 671, loss = 0.24539970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 672/1600\n",
      "Iteration 672, loss = 0.24568040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 673/1600\n",
      "Iteration 673, loss = 0.24536883\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 674/1600\n",
      "Iteration 674, loss = 0.24563867\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 675/1600\n",
      "Iteration 675, loss = 0.24532991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 676/1600\n",
      "Iteration 676, loss = 0.24562074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 677/1600\n",
      "Iteration 677, loss = 0.24532295\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 678/1600\n",
      "Iteration 678, loss = 0.24559608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 679/1600\n",
      "Iteration 679, loss = 0.24532415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 680/1600\n",
      "Iteration 680, loss = 0.24559959\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 681/1600\n",
      "Iteration 681, loss = 0.24529600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 682/1600\n",
      "Iteration 682, loss = 0.24558726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 683/1600\n",
      "Iteration 683, loss = 0.24528734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 684/1600\n",
      "Iteration 684, loss = 0.24555343\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 685/1600\n",
      "Iteration 685, loss = 0.24526151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 686/1600\n",
      "Iteration 686, loss = 0.24554885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 687/1600\n",
      "Iteration 687, loss = 0.24531090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 688/1600\n",
      "Iteration 688, loss = 0.24562732\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 689/1600\n",
      "Iteration 689, loss = 0.24536407\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 690/1600\n",
      "Iteration 690, loss = 0.24568709\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 691/1600\n",
      "Iteration 691, loss = 0.24540022\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 692/1600\n",
      "Iteration 692, loss = 0.24572162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 693/1600\n",
      "Iteration 693, loss = 0.24538688\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 694/1600\n",
      "Iteration 694, loss = 0.24569161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 695/1600\n",
      "Iteration 695, loss = 0.24535970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 696/1600\n",
      "Iteration 696, loss = 0.24564433\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 697/1600\n",
      "Iteration 697, loss = 0.24531646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 698/1600\n",
      "Iteration 698, loss = 0.24555541\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 699/1600\n",
      "Iteration 699, loss = 0.24524697\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 700/1600\n",
      "Iteration 700, loss = 0.24548905\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 701/1600\n",
      "Iteration 701, loss = 0.24519243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 702/1600\n",
      "Iteration 702, loss = 0.24543600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 703/1600\n",
      "Iteration 703, loss = 0.24515350\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 704/1600\n",
      "Iteration 704, loss = 0.24541287\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 705/1600\n",
      "Iteration 705, loss = 0.24513151\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 706/1600\n",
      "Iteration 706, loss = 0.24538184\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 707/1600\n",
      "Iteration 707, loss = 0.24512003\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 708/1600\n",
      "Iteration 708, loss = 0.24537447\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 709/1600\n",
      "Iteration 709, loss = 0.24509921\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 710/1600\n",
      "Iteration 710, loss = 0.24533999\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 711/1600\n",
      "Iteration 711, loss = 0.24508144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 712/1600\n",
      "Iteration 712, loss = 0.24533976\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 713/1600\n",
      "Iteration 713, loss = 0.24510282\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 714/1600\n",
      "Iteration 714, loss = 0.24538931\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 715/1600\n",
      "Iteration 715, loss = 0.24511416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 716/1600\n",
      "Iteration 716, loss = 0.24538146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 717/1600\n",
      "Iteration 717, loss = 0.24511314\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 718/1600\n",
      "Iteration 718, loss = 0.24536579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 719/1600\n",
      "Iteration 719, loss = 0.24508454\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 720/1600\n",
      "Iteration 720, loss = 0.24531197\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 721/1600\n",
      "Iteration 721, loss = 0.24504285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 722/1600\n",
      "Iteration 722, loss = 0.24527761\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 723/1600\n",
      "Iteration 723, loss = 0.24502689\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 724/1600\n",
      "Iteration 724, loss = 0.24526725\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 725/1600\n",
      "Iteration 725, loss = 0.24501970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 726/1600\n",
      "Iteration 726, loss = 0.24525820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 727/1600\n",
      "Iteration 727, loss = 0.24500458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 728/1600\n",
      "Iteration 728, loss = 0.24529748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 729/1600\n",
      "Iteration 729, loss = 0.24505898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 730/1600\n",
      "Iteration 730, loss = 0.24532395\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 731/1600\n",
      "Iteration 731, loss = 0.24508027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 732/1600\n",
      "Iteration 732, loss = 0.24533658\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 733/1600\n",
      "Iteration 733, loss = 0.24505836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 734/1600\n",
      "Iteration 734, loss = 0.24530698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 735/1600\n",
      "Iteration 735, loss = 0.24511733\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 736/1600\n",
      "Iteration 736, loss = 0.24538033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 737/1600\n",
      "Iteration 737, loss = 0.24508427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 738/1600\n",
      "Iteration 738, loss = 0.24530622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 739/1600\n",
      "Iteration 739, loss = 0.24503142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 740/1600\n",
      "Iteration 740, loss = 0.24524986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 741/1600\n",
      "Iteration 741, loss = 0.24498982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 742/1600\n",
      "Iteration 742, loss = 0.24522240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 743/1600\n",
      "Iteration 743, loss = 0.24496190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 744/1600\n",
      "Iteration 744, loss = 0.24520233\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 745/1600\n",
      "Iteration 745, loss = 0.24498890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 746/1600\n",
      "Iteration 746, loss = 0.24522149\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 747/1600\n",
      "Iteration 747, loss = 0.24495683\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 748/1600\n",
      "Iteration 748, loss = 0.24519989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 749/1600\n",
      "Iteration 749, loss = 0.24494514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 750/1600\n",
      "Iteration 750, loss = 0.24518281\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 751/1600\n",
      "Iteration 751, loss = 0.24492674\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 752/1600\n",
      "Iteration 752, loss = 0.24514936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 753/1600\n",
      "Iteration 753, loss = 0.24491502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 754/1600\n",
      "Iteration 754, loss = 0.24513823\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 755/1600\n",
      "Iteration 755, loss = 0.24489323\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 756/1600\n",
      "Iteration 756, loss = 0.24511565\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 757/1600\n",
      "Iteration 757, loss = 0.24487578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 758/1600\n",
      "Iteration 758, loss = 0.24509794\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 759/1600\n",
      "Iteration 759, loss = 0.24488509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 760/1600\n",
      "Iteration 760, loss = 0.24512742\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 761/1600\n",
      "Iteration 761, loss = 0.24490270\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 762/1600\n",
      "Iteration 762, loss = 0.24514258\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 763/1600\n",
      "Iteration 763, loss = 0.24492995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 764/1600\n",
      "Iteration 764, loss = 0.24518880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 765/1600\n",
      "Iteration 765, loss = 0.24493846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 766/1600\n",
      "Iteration 766, loss = 0.24519578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 767/1600\n",
      "Iteration 767, loss = 0.24495926\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 768/1600\n",
      "Iteration 768, loss = 0.24521913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 769/1600\n",
      "Iteration 769, loss = 0.24497079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 770/1600\n",
      "Iteration 770, loss = 0.24522216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 771/1600\n",
      "Iteration 771, loss = 0.24500679\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 772/1600\n",
      "Iteration 772, loss = 0.24528932\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 773/1600\n",
      "Iteration 773, loss = 0.24503938\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 774/1600\n",
      "Iteration 774, loss = 0.24530307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 775/1600\n",
      "Iteration 775, loss = 0.24503017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 776/1600\n",
      "Iteration 776, loss = 0.24528964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 777/1600\n",
      "Iteration 777, loss = 0.24501307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 778/1600\n",
      "Iteration 778, loss = 0.24525779\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 779/1600\n",
      "Iteration 779, loss = 0.24498059\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 780/1600\n",
      "Iteration 780, loss = 0.24521307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 781/1600\n",
      "Iteration 781, loss = 0.24494672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 782/1600\n",
      "Iteration 782, loss = 0.24517388\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 783/1600\n",
      "Iteration 783, loss = 0.24491622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 784/1600\n",
      "Iteration 784, loss = 0.24514255\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 785/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 785, loss = 0.24490474\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 786/1600\n",
      "Iteration 786, loss = 0.24513455\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 787/1600\n",
      "Iteration 787, loss = 0.24487467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 788/1600\n",
      "Iteration 788, loss = 0.24510711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 789/1600\n",
      "Iteration 789, loss = 0.24486615\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 790/1600\n",
      "Iteration 790, loss = 0.24509432\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 791/1600\n",
      "Iteration 791, loss = 0.24483974\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 792/1600\n",
      "Iteration 792, loss = 0.24505675\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 793/1600\n",
      "Iteration 793, loss = 0.24480721\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 794/1600\n",
      "Iteration 794, loss = 0.24500821\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 795/1600\n",
      "Iteration 795, loss = 0.24479466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 796/1600\n",
      "Iteration 796, loss = 0.24502125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 797/1600\n",
      "Iteration 797, loss = 0.24478464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 798/1600\n",
      "Iteration 798, loss = 0.24500232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 799/1600\n",
      "Iteration 799, loss = 0.24477279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 800/1600\n",
      "Iteration 800, loss = 0.24496330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 801/1600\n",
      "Iteration 801, loss = 0.24472654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 802/1600\n",
      "Iteration 802, loss = 0.24493843\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 803/1600\n",
      "Iteration 803, loss = 0.24471369\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 804/1600\n",
      "Iteration 804, loss = 0.24491383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 805/1600\n",
      "Iteration 805, loss = 0.24469994\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 806/1600\n",
      "Iteration 806, loss = 0.24490827\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 807/1600\n",
      "Iteration 807, loss = 0.24470452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 808/1600\n",
      "Iteration 808, loss = 0.24491115\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 809/1600\n",
      "Iteration 809, loss = 0.24469538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 810/1600\n",
      "Iteration 810, loss = 0.24490354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 811/1600\n",
      "Iteration 811, loss = 0.24468318\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 812/1600\n",
      "Iteration 812, loss = 0.24488918\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 813/1600\n",
      "Iteration 813, loss = 0.24467523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 814/1600\n",
      "Iteration 814, loss = 0.24488133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 815/1600\n",
      "Iteration 815, loss = 0.24467456\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 816/1600\n",
      "Iteration 816, loss = 0.24488435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 817/1600\n",
      "Iteration 817, loss = 0.24469251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 818/1600\n",
      "Iteration 818, loss = 0.24492174\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 819/1600\n",
      "Iteration 819, loss = 0.24470173\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 820/1600\n",
      "Iteration 820, loss = 0.24491075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 821/1600\n",
      "Iteration 821, loss = 0.24469203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 822/1600\n",
      "Iteration 822, loss = 0.24491444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 823/1600\n",
      "Iteration 823, loss = 0.24470113\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 824/1600\n",
      "Iteration 824, loss = 0.24490566\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 825/1600\n",
      "Iteration 825, loss = 0.24469622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 826/1600\n",
      "Iteration 826, loss = 0.24491405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 827/1600\n",
      "Iteration 827, loss = 0.24469539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 828/1600\n",
      "Iteration 828, loss = 0.24490714\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 829/1600\n",
      "Iteration 829, loss = 0.24469068\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 830/1600\n",
      "Iteration 830, loss = 0.24487591\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 831/1600\n",
      "Iteration 831, loss = 0.24465440\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 832/1600\n",
      "Iteration 832, loss = 0.24486462\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 833/1600\n",
      "Iteration 833, loss = 0.24466863\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 834/1600\n",
      "Iteration 834, loss = 0.24488703\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 835/1600\n",
      "Iteration 835, loss = 0.24466652\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 836/1600\n",
      "Iteration 836, loss = 0.24489160\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 837/1600\n",
      "Iteration 837, loss = 0.24470034\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 838/1600\n",
      "Iteration 838, loss = 0.24493335\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 839/1600\n",
      "Iteration 839, loss = 0.24471881\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 840/1600\n",
      "Iteration 840, loss = 0.24493630\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 841/1600\n",
      "Iteration 841, loss = 0.24472011\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 842/1600\n",
      "Iteration 842, loss = 0.24493880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 843/1600\n",
      "Iteration 843, loss = 0.24470696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 844/1600\n",
      "Iteration 844, loss = 0.24492260\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 845/1600\n",
      "Iteration 845, loss = 0.24469487\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 846/1600\n",
      "Iteration 846, loss = 0.24490509\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 847/1600\n",
      "Iteration 847, loss = 0.24471387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 848/1600\n",
      "Iteration 848, loss = 0.24492556\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 849/1600\n",
      "Iteration 849, loss = 0.24469083\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 850/1600\n",
      "Iteration 850, loss = 0.24494568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 851/1600\n",
      "Iteration 851, loss = 0.24471245\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 852/1600\n",
      "Iteration 852, loss = 0.24492747\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 853/1600\n",
      "Iteration 853, loss = 0.24468380\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 854/1600\n",
      "Iteration 854, loss = 0.24489336\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 855/1600\n",
      "Iteration 855, loss = 0.24468303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 856/1600\n",
      "Iteration 856, loss = 0.24489138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 857/1600\n",
      "Iteration 857, loss = 0.24466434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 858/1600\n",
      "Iteration 858, loss = 0.24485057\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 859/1600\n",
      "Iteration 859, loss = 0.24463116\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 860/1600\n",
      "Iteration 860, loss = 0.24483452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 861/1600\n",
      "Iteration 861, loss = 0.24461640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 862/1600\n",
      "Iteration 862, loss = 0.24481638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 863/1600\n",
      "Iteration 863, loss = 0.24460203\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 864/1600\n",
      "Iteration 864, loss = 0.24478005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 865/1600\n",
      "Iteration 865, loss = 0.24456911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 866/1600\n",
      "Iteration 866, loss = 0.24475367\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 867/1600\n",
      "Iteration 867, loss = 0.24454055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 868/1600\n",
      "Iteration 868, loss = 0.24472908\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 869/1600\n",
      "Iteration 869, loss = 0.24453523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 870/1600\n",
      "Iteration 870, loss = 0.24471303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 871/1600\n",
      "Iteration 871, loss = 0.24450927\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 872/1600\n",
      "Iteration 872, loss = 0.24469754\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 873/1600\n",
      "Iteration 873, loss = 0.24453793\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 874/1600\n",
      "Iteration 874, loss = 0.24473706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 875/1600\n",
      "Iteration 875, loss = 0.24453117\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 876/1600\n",
      "Iteration 876, loss = 0.24472696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 877/1600\n",
      "Iteration 877, loss = 0.24454095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 878/1600\n",
      "Iteration 878, loss = 0.24474387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 879/1600\n",
      "Iteration 879, loss = 0.24454201\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 880/1600\n",
      "Iteration 880, loss = 0.24473682\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 881/1600\n",
      "Iteration 881, loss = 0.24452718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 882/1600\n",
      "Iteration 882, loss = 0.24471746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 883/1600\n",
      "Iteration 883, loss = 0.24451484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 884/1600\n",
      "Iteration 884, loss = 0.24470588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 885/1600\n",
      "Iteration 885, loss = 0.24451000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 886/1600\n",
      "Iteration 886, loss = 0.24470033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 887/1600\n",
      "Iteration 887, loss = 0.24449342\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 888/1600\n",
      "Iteration 888, loss = 0.24466535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 889/1600\n",
      "Iteration 889, loss = 0.24447719\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 890/1600\n",
      "Iteration 890, loss = 0.24465587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 891/1600\n",
      "Iteration 891, loss = 0.24446572\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 892/1600\n",
      "Iteration 892, loss = 0.24465608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 893/1600\n",
      "Iteration 893, loss = 0.24446907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 894/1600\n",
      "Iteration 894, loss = 0.24466499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 895/1600\n",
      "Iteration 895, loss = 0.24447370\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 896/1600\n",
      "Iteration 896, loss = 0.24465589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 897/1600\n",
      "Iteration 897, loss = 0.24445458\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 898/1600\n",
      "Iteration 898, loss = 0.24462263\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 899/1600\n",
      "Iteration 899, loss = 0.24444848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 900/1600\n",
      "Iteration 900, loss = 0.24463945\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 901/1600\n",
      "Iteration 901, loss = 0.24444290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 902/1600\n",
      "Iteration 902, loss = 0.24463072\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 903/1600\n",
      "Iteration 903, loss = 0.24446074\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 904/1600\n",
      "Iteration 904, loss = 0.24465108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 905/1600\n",
      "Iteration 905, loss = 0.24446731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 906/1600\n",
      "Iteration 906, loss = 0.24466897\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 907/1600\n",
      "Iteration 907, loss = 0.24447054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 908/1600\n",
      "Iteration 908, loss = 0.24465248\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 909/1600\n",
      "Iteration 909, loss = 0.24446911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 910/1600\n",
      "Iteration 910, loss = 0.24466590\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 911/1600\n",
      "Iteration 911, loss = 0.24447436\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 912/1600\n",
      "Iteration 912, loss = 0.24466697\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 913/1600\n",
      "Iteration 913, loss = 0.24446832\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 914/1600\n",
      "Iteration 914, loss = 0.24466912\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 915/1600\n",
      "Iteration 915, loss = 0.24447485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 916/1600\n",
      "Iteration 916, loss = 0.24466564\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 917/1600\n",
      "Iteration 917, loss = 0.24446715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 918/1600\n",
      "Iteration 918, loss = 0.24464400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 919/1600\n",
      "Iteration 919, loss = 0.24445896\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 920/1600\n",
      "Iteration 920, loss = 0.24464448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 921/1600\n",
      "Iteration 921, loss = 0.24444220\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 922/1600\n",
      "Iteration 922, loss = 0.24462933\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 923/1600\n",
      "Iteration 923, loss = 0.24443161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 924/1600\n",
      "Iteration 924, loss = 0.24460117\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 925/1600\n",
      "Iteration 925, loss = 0.24441696\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 926/1600\n",
      "Iteration 926, loss = 0.24458656\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 927/1600\n",
      "Iteration 927, loss = 0.24440140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 928/1600\n",
      "Iteration 928, loss = 0.24456425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 929/1600\n",
      "Iteration 929, loss = 0.24437420\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 930/1600\n",
      "Iteration 930, loss = 0.24454313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 931/1600\n",
      "Iteration 931, loss = 0.24435561\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 932/1600\n",
      "Iteration 932, loss = 0.24452578\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 933/1600\n",
      "Iteration 933, loss = 0.24434473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 934/1600\n",
      "Iteration 934, loss = 0.24451631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 935/1600\n",
      "Iteration 935, loss = 0.24435343\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 936/1600\n",
      "Iteration 936, loss = 0.24453880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 937/1600\n",
      "Iteration 937, loss = 0.24437053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 938/1600\n",
      "Iteration 938, loss = 0.24454972\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 939/1600\n",
      "Iteration 939, loss = 0.24437373\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 940/1600\n",
      "Iteration 940, loss = 0.24456168\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 941/1600\n",
      "Iteration 941, loss = 0.24437313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 942/1600\n",
      "Iteration 942, loss = 0.24455613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 943/1600\n",
      "Iteration 943, loss = 0.24438537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 944/1600\n",
      "Iteration 944, loss = 0.24456232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 945/1600\n",
      "Iteration 945, loss = 0.24437776\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 946/1600\n",
      "Iteration 946, loss = 0.24456111\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 947/1600\n",
      "Iteration 947, loss = 0.24437755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 948/1600\n",
      "Iteration 948, loss = 0.24455546\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 949/1600\n",
      "Iteration 949, loss = 0.24437190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 950/1600\n",
      "Iteration 950, loss = 0.24454514\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 951/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 951, loss = 0.24436127\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 952/1600\n",
      "Iteration 952, loss = 0.24453659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 953/1600\n",
      "Iteration 953, loss = 0.24435046\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 954/1600\n",
      "Iteration 954, loss = 0.24451668\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 955/1600\n",
      "Iteration 955, loss = 0.24433348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 956/1600\n",
      "Iteration 956, loss = 0.24449144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 957/1600\n",
      "Iteration 957, loss = 0.24434699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 958/1600\n",
      "Iteration 958, loss = 0.24451913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 959/1600\n",
      "Iteration 959, loss = 0.24433453\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 960/1600\n",
      "Iteration 960, loss = 0.24452053\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 961/1600\n",
      "Iteration 961, loss = 0.24433328\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 962/1600\n",
      "Iteration 962, loss = 0.24451365\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 963/1600\n",
      "Iteration 963, loss = 0.24434773\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 964/1600\n",
      "Iteration 964, loss = 0.24452812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 965/1600\n",
      "Iteration 965, loss = 0.24434065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 966/1600\n",
      "Iteration 966, loss = 0.24450346\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 967/1600\n",
      "Iteration 967, loss = 0.24433387\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 968/1600\n",
      "Iteration 968, loss = 0.24450817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 969/1600\n",
      "Iteration 969, loss = 0.24432125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 970/1600\n",
      "Iteration 970, loss = 0.24447305\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 971/1600\n",
      "Iteration 971, loss = 0.24428695\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 972/1600\n",
      "Iteration 972, loss = 0.24444286\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 973/1600\n",
      "Iteration 973, loss = 0.24426684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 974/1600\n",
      "Iteration 974, loss = 0.24442138\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 975/1600\n",
      "Iteration 975, loss = 0.24425075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 976/1600\n",
      "Iteration 976, loss = 0.24441132\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 977/1600\n",
      "Iteration 977, loss = 0.24423489\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 978/1600\n",
      "Iteration 978, loss = 0.24437878\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 979/1600\n",
      "Iteration 979, loss = 0.24420718\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 980/1600\n",
      "Iteration 980, loss = 0.24435430\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 981/1600\n",
      "Iteration 981, loss = 0.24418583\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 982/1600\n",
      "Iteration 982, loss = 0.24434257\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 983/1600\n",
      "Iteration 983, loss = 0.24418602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 984/1600\n",
      "Iteration 984, loss = 0.24432428\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 985/1600\n",
      "Iteration 985, loss = 0.24415143\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 986/1600\n",
      "Iteration 986, loss = 0.24428356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 987/1600\n",
      "Iteration 987, loss = 0.24412359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 988/1600\n",
      "Iteration 988, loss = 0.24425589\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 989/1600\n",
      "Iteration 989, loss = 0.24409854\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 990/1600\n",
      "Iteration 990, loss = 0.24426485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 991/1600\n",
      "Iteration 991, loss = 0.24414694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 992/1600\n",
      "Iteration 992, loss = 0.24430093\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 993/1600\n",
      "Iteration 993, loss = 0.24414871\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 994/1600\n",
      "Iteration 994, loss = 0.24429150\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 995/1600\n",
      "Iteration 995, loss = 0.24413212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 996/1600\n",
      "Iteration 996, loss = 0.24427797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 997/1600\n",
      "Iteration 997, loss = 0.24412716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 998/1600\n",
      "Iteration 998, loss = 0.24426930\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 999/1600\n",
      "Iteration 999, loss = 0.24410771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1000/1600\n",
      "Iteration 1000, loss = 0.24423208\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1001/1600\n",
      "Iteration 1001, loss = 0.24408791\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1002/1600\n",
      "Iteration 1002, loss = 0.24421372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1003/1600\n",
      "Iteration 1003, loss = 0.24406013\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1004/1600\n",
      "Iteration 1004, loss = 0.24419189\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1005/1600\n",
      "Iteration 1005, loss = 0.24404597\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1006/1600\n",
      "Iteration 1006, loss = 0.24416877\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1007/1600\n",
      "Iteration 1007, loss = 0.24402418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1008/1600\n",
      "Iteration 1008, loss = 0.24415145\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1009/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1009, loss = 0.24401666\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1010/1600\n",
      "Iteration 1010, loss = 0.24415213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1011/1600\n",
      "Iteration 1011, loss = 0.24401852\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1012/1600\n",
      "Iteration 1012, loss = 0.24415759\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1013/1600\n",
      "Iteration 1013, loss = 0.24403882\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1014/1600\n",
      "Iteration 1014, loss = 0.24418943\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1015/1600\n",
      "Iteration 1015, loss = 0.24405631\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1016/1600\n",
      "Iteration 1016, loss = 0.24420624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1017/1600\n",
      "Iteration 1017, loss = 0.24407001\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1018/1600\n",
      "Iteration 1018, loss = 0.24421731\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1019/1600\n",
      "Iteration 1019, loss = 0.24407903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1020/1600\n",
      "Iteration 1020, loss = 0.24422927\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1021/1600\n",
      "Iteration 1021, loss = 0.24410298\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1022/1600\n",
      "Iteration 1022, loss = 0.24426715\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1023/1600\n",
      "Iteration 1023, loss = 0.24414190\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1024/1600\n",
      "Iteration 1024, loss = 0.24430434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1025/1600\n",
      "Iteration 1025, loss = 0.24415188\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1026/1600\n",
      "Iteration 1026, loss = 0.24430946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1027/1600\n",
      "Iteration 1027, loss = 0.24415793\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1028/1600\n",
      "Iteration 1028, loss = 0.24431473\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1029/1600\n",
      "Iteration 1029, loss = 0.24416310\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1030/1600\n",
      "Iteration 1030, loss = 0.24432272\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1031/1600\n",
      "Iteration 1031, loss = 0.24416569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1032/1600\n",
      "Iteration 1032, loss = 0.24433045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1033/1600\n",
      "Iteration 1033, loss = 0.24418456\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1034/1600\n",
      "Iteration 1034, loss = 0.24436112\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1035/1600\n",
      "Iteration 1035, loss = 0.24420231\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1036/1600\n",
      "Iteration 1036, loss = 0.24436650\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1037/1600\n",
      "Iteration 1037, loss = 0.24420162\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1038/1600\n",
      "Iteration 1038, loss = 0.24437240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1039/1600\n",
      "Iteration 1039, loss = 0.24420582\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1040/1600\n",
      "Iteration 1040, loss = 0.24436815\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1041/1600\n",
      "Iteration 1041, loss = 0.24422061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1042/1600\n",
      "Iteration 1042, loss = 0.24439216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1043/1600\n",
      "Iteration 1043, loss = 0.24422442\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1044/1600\n",
      "Iteration 1044, loss = 0.24440227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1045/1600\n",
      "Iteration 1045, loss = 0.24423486\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1046/1600\n",
      "Iteration 1046, loss = 0.24440466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1047/1600\n",
      "Iteration 1047, loss = 0.24423265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1048/1600\n",
      "Iteration 1048, loss = 0.24440484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1049/1600\n",
      "Iteration 1049, loss = 0.24422944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1050/1600\n",
      "Iteration 1050, loss = 0.24438797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1051/1600\n",
      "Iteration 1051, loss = 0.24421247\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1052/1600\n",
      "Iteration 1052, loss = 0.24437202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1053/1600\n",
      "Iteration 1053, loss = 0.24420064\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1054/1600\n",
      "Iteration 1054, loss = 0.24436803\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1055/1600\n",
      "Iteration 1055, loss = 0.24422155\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1056/1600\n",
      "Iteration 1056, loss = 0.24438181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1057/1600\n",
      "Iteration 1057, loss = 0.24421958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1058/1600\n",
      "Iteration 1058, loss = 0.24440256\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1059/1600\n",
      "Iteration 1059, loss = 0.24422933\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1060/1600\n",
      "Iteration 1060, loss = 0.24439910\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1061/1600\n",
      "Iteration 1061, loss = 0.24421813\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1062/1600\n",
      "Iteration 1062, loss = 0.24436940\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1063/1600\n",
      "Iteration 1063, loss = 0.24418821\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1064/1600\n",
      "Iteration 1064, loss = 0.24434262\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1065/1600\n",
      "Iteration 1065, loss = 0.24418569\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1066/1600\n",
      "Iteration 1066, loss = 0.24433840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1067/1600\n",
      "Iteration 1067, loss = 0.24416041\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1068/1600\n",
      "Iteration 1068, loss = 0.24430102\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1069/1600\n",
      "Iteration 1069, loss = 0.24411991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1070/1600\n",
      "Iteration 1070, loss = 0.24424035\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1071/1600\n",
      "Iteration 1071, loss = 0.24406368\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1072/1600\n",
      "Iteration 1072, loss = 0.24419054\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1073/1600\n",
      "Iteration 1073, loss = 0.24404687\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1074/1600\n",
      "Iteration 1074, loss = 0.24417513\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1075/1600\n",
      "Iteration 1075, loss = 0.24400682\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1076/1600\n",
      "Iteration 1076, loss = 0.24411923\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1077/1600\n",
      "Iteration 1077, loss = 0.24395634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1078/1600\n",
      "Iteration 1078, loss = 0.24406213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1079/1600\n",
      "Iteration 1079, loss = 0.24391240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1080/1600\n",
      "Iteration 1080, loss = 0.24400992\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1081/1600\n",
      "Iteration 1081, loss = 0.24386813\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1082/1600\n",
      "Iteration 1082, loss = 0.24395820\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1083/1600\n",
      "Iteration 1083, loss = 0.24382782\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1084/1600\n",
      "Iteration 1084, loss = 0.24391029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1085/1600\n",
      "Iteration 1085, loss = 0.24378331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1086/1600\n",
      "Iteration 1086, loss = 0.24387379\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1087/1600\n",
      "Iteration 1087, loss = 0.24375801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1088/1600\n",
      "Iteration 1088, loss = 0.24385392\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1089/1600\n",
      "Iteration 1089, loss = 0.24374282\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1090/1600\n",
      "Iteration 1090, loss = 0.24384123\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1091/1600\n",
      "Iteration 1091, loss = 0.24373646\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1092/1600\n",
      "Iteration 1092, loss = 0.24383836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1093/1600\n",
      "Iteration 1093, loss = 0.24374108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1094/1600\n",
      "Iteration 1094, loss = 0.24383964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1095/1600\n",
      "Iteration 1095, loss = 0.24373562\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1096/1600\n",
      "Iteration 1096, loss = 0.24384089\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1097/1600\n",
      "Iteration 1097, loss = 0.24374044\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1098/1600\n",
      "Iteration 1098, loss = 0.24384835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1099/1600\n",
      "Iteration 1099, loss = 0.24374522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1100/1600\n",
      "Iteration 1100, loss = 0.24384808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1101/1600\n",
      "Iteration 1101, loss = 0.24375604\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1102/1600\n",
      "Iteration 1102, loss = 0.24386999\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1103/1600\n",
      "Iteration 1103, loss = 0.24376403\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1104/1600\n",
      "Iteration 1104, loss = 0.24391114\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1105/1600\n",
      "Iteration 1105, loss = 0.24384920\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1106/1600\n",
      "Iteration 1106, loss = 0.24398810\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1107/1600\n",
      "Iteration 1107, loss = 0.24388040\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1108/1600\n",
      "Iteration 1108, loss = 0.24401934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1109/1600\n",
      "Iteration 1109, loss = 0.24391736\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1110/1600\n",
      "Iteration 1110, loss = 0.24406278\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1111/1600\n",
      "Iteration 1111, loss = 0.24393919\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1112/1600\n",
      "Iteration 1112, loss = 0.24408779\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1113/1600\n",
      "Iteration 1113, loss = 0.24395537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1114/1600\n",
      "Iteration 1114, loss = 0.24410316\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1115/1600\n",
      "Iteration 1115, loss = 0.24397213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1116/1600\n",
      "Iteration 1116, loss = 0.24411444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1117/1600\n",
      "Iteration 1117, loss = 0.24397466\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1118/1600\n",
      "Iteration 1118, loss = 0.24410695\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1119/1600\n",
      "Iteration 1119, loss = 0.24396735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1120/1600\n",
      "Iteration 1120, loss = 0.24410817\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1121/1600\n",
      "Iteration 1121, loss = 0.24397485\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1122/1600\n",
      "Iteration 1122, loss = 0.24411232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1123/1600\n",
      "Iteration 1123, loss = 0.24398613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1124/1600\n",
      "Iteration 1124, loss = 0.24413860\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1125/1600\n",
      "Iteration 1125, loss = 0.24399694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1126/1600\n",
      "Iteration 1126, loss = 0.24414253\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1127/1600\n",
      "Iteration 1127, loss = 0.24400635\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1128/1600\n",
      "Iteration 1128, loss = 0.24416467\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1129/1600\n",
      "Iteration 1129, loss = 0.24402493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1130/1600\n",
      "Iteration 1130, loss = 0.24418744\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1131/1600\n",
      "Iteration 1131, loss = 0.24404427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1132/1600\n",
      "Iteration 1132, loss = 0.24420783\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1133/1600\n",
      "Iteration 1133, loss = 0.24407164\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1134/1600\n",
      "Iteration 1134, loss = 0.24422947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1135/1600\n",
      "Iteration 1135, loss = 0.24407356\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1136/1600\n",
      "Iteration 1136, loss = 0.24422413\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1137/1600\n",
      "Iteration 1137, loss = 0.24406634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1138/1600\n",
      "Iteration 1138, loss = 0.24422294\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1139/1600\n",
      "Iteration 1139, loss = 0.24406690\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1140/1600\n",
      "Iteration 1140, loss = 0.24421845\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1141/1600\n",
      "Iteration 1141, loss = 0.24407119\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1142/1600\n",
      "Iteration 1142, loss = 0.24422895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1143/1600\n",
      "Iteration 1143, loss = 0.24406419\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1144/1600\n",
      "Iteration 1144, loss = 0.24421415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1145/1600\n",
      "Iteration 1145, loss = 0.24405882\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1146/1600\n",
      "Iteration 1146, loss = 0.24420626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1147/1600\n",
      "Iteration 1147, loss = 0.24404833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1148/1600\n",
      "Iteration 1148, loss = 0.24418713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1149/1600\n",
      "Iteration 1149, loss = 0.24403354\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1150/1600\n",
      "Iteration 1150, loss = 0.24417829\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1151/1600\n",
      "Iteration 1151, loss = 0.24401716\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1152/1600\n",
      "Iteration 1152, loss = 0.24414658\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1153/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1153, loss = 0.24400179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1154/1600\n",
      "Iteration 1154, loss = 0.24414194\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1155/1600\n",
      "Iteration 1155, loss = 0.24398911\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1156/1600\n",
      "Iteration 1156, loss = 0.24412523\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1157/1600\n",
      "Iteration 1157, loss = 0.24397062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1158/1600\n",
      "Iteration 1158, loss = 0.24410598\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1159/1600\n",
      "Iteration 1159, loss = 0.24395469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1160/1600\n",
      "Iteration 1160, loss = 0.24408846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1161/1600\n",
      "Iteration 1161, loss = 0.24394140\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1162/1600\n",
      "Iteration 1162, loss = 0.24406504\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1163/1600\n",
      "Iteration 1163, loss = 0.24391684\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1164/1600\n",
      "Iteration 1164, loss = 0.24403351\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1165/1600\n",
      "Iteration 1165, loss = 0.24389078\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1166/1600\n",
      "Iteration 1166, loss = 0.24399970\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1167/1600\n",
      "Iteration 1167, loss = 0.24386378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1168/1600\n",
      "Iteration 1168, loss = 0.24397692\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1169/1600\n",
      "Iteration 1169, loss = 0.24384986\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1170/1600\n",
      "Iteration 1170, loss = 0.24395626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1171/1600\n",
      "Iteration 1171, loss = 0.24382146\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1172/1600\n",
      "Iteration 1172, loss = 0.24393265\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1173/1600\n",
      "Iteration 1173, loss = 0.24380418\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1174/1600\n",
      "Iteration 1174, loss = 0.24391987\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1175/1600\n",
      "Iteration 1175, loss = 0.24379898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1176/1600\n",
      "Iteration 1176, loss = 0.24390625\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1177/1600\n",
      "Iteration 1177, loss = 0.24378739\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1178/1600\n",
      "Iteration 1178, loss = 0.24389482\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1179/1600\n",
      "Iteration 1179, loss = 0.24377763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1180/1600\n",
      "Iteration 1180, loss = 0.24388838\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1181/1600\n",
      "Iteration 1181, loss = 0.24376893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1182/1600\n",
      "Iteration 1182, loss = 0.24388004\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1183/1600\n",
      "Iteration 1183, loss = 0.24378075\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1184/1600\n",
      "Iteration 1184, loss = 0.24389586\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1185/1600\n",
      "Iteration 1185, loss = 0.24379250\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1186/1600\n",
      "Iteration 1186, loss = 0.24391478\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1187/1600\n",
      "Iteration 1187, loss = 0.24380047\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1188/1600\n",
      "Iteration 1188, loss = 0.24391291\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1189/1600\n",
      "Iteration 1189, loss = 0.24379029\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1190/1600\n",
      "Iteration 1190, loss = 0.24389748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1191/1600\n",
      "Iteration 1191, loss = 0.24377638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1192/1600\n",
      "Iteration 1192, loss = 0.24388359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1193/1600\n",
      "Iteration 1193, loss = 0.24376642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1194/1600\n",
      "Iteration 1194, loss = 0.24388331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1195/1600\n",
      "Iteration 1195, loss = 0.24376507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1196/1600\n",
      "Iteration 1196, loss = 0.24387831\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1197/1600\n",
      "Iteration 1197, loss = 0.24381103\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1198/1600\n",
      "Iteration 1198, loss = 0.24394919\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1199/1600\n",
      "Iteration 1199, loss = 0.24383469\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1200/1600\n",
      "Iteration 1200, loss = 0.24396862\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1201/1600\n",
      "Iteration 1201, loss = 0.24383522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1202/1600\n",
      "Iteration 1202, loss = 0.24396359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1203/1600\n",
      "Iteration 1203, loss = 0.24383735\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1204/1600\n",
      "Iteration 1204, loss = 0.24396814\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1205/1600\n",
      "Iteration 1205, loss = 0.24385187\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1206/1600\n",
      "Iteration 1206, loss = 0.24397869\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1207/1600\n",
      "Iteration 1207, loss = 0.24384799\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1208/1600\n",
      "Iteration 1208, loss = 0.24397179\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1209/1600\n",
      "Iteration 1209, loss = 0.24384259\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1210/1600\n",
      "Iteration 1210, loss = 0.24395966\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1211/1600\n",
      "Iteration 1211, loss = 0.24382812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1212/1600\n",
      "Iteration 1212, loss = 0.24394800\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1213/1600\n",
      "Iteration 1213, loss = 0.24382077\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1214/1600\n",
      "Iteration 1214, loss = 0.24393695\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1215/1600\n",
      "Iteration 1215, loss = 0.24381025\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1216/1600\n",
      "Iteration 1216, loss = 0.24392132\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1217/1600\n",
      "Iteration 1217, loss = 0.24379435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1218/1600\n",
      "Iteration 1218, loss = 0.24391195\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1219/1600\n",
      "Iteration 1219, loss = 0.24379290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1220/1600\n",
      "Iteration 1220, loss = 0.24390956\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1221/1600\n",
      "Iteration 1221, loss = 0.24380144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1222/1600\n",
      "Iteration 1222, loss = 0.24392723\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1223/1600\n",
      "Iteration 1223, loss = 0.24380321\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1224/1600\n",
      "Iteration 1224, loss = 0.24392805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1225/1600\n",
      "Iteration 1225, loss = 0.24381055\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1226/1600\n",
      "Iteration 1226, loss = 0.24393416\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1227/1600\n",
      "Iteration 1227, loss = 0.24380919\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1228/1600\n",
      "Iteration 1228, loss = 0.24393199\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1229/1600\n",
      "Iteration 1229, loss = 0.24380663\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1230/1600\n",
      "Iteration 1230, loss = 0.24392273\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1231/1600\n",
      "Iteration 1231, loss = 0.24379835\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1232/1600\n",
      "Iteration 1232, loss = 0.24390816\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1233/1600\n",
      "Iteration 1233, loss = 0.24376580\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1234/1600\n",
      "Iteration 1234, loss = 0.24386198\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1235/1600\n",
      "Iteration 1235, loss = 0.24373098\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1236/1600\n",
      "Iteration 1236, loss = 0.24382340\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1237/1600\n",
      "Iteration 1237, loss = 0.24368246\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1238/1600\n",
      "Iteration 1238, loss = 0.24374194\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1239/1600\n",
      "Iteration 1239, loss = 0.24361202\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1240/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1240, loss = 0.24366963\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1241/1600\n",
      "Iteration 1241, loss = 0.24355312\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1242/1600\n",
      "Iteration 1242, loss = 0.24363812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1243/1600\n",
      "Iteration 1243, loss = 0.24353161\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1244/1600\n",
      "Iteration 1244, loss = 0.24359239\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1245/1600\n",
      "Iteration 1245, loss = 0.24348958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1246/1600\n",
      "Iteration 1246, loss = 0.24354686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1247/1600\n",
      "Iteration 1247, loss = 0.24345378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1248/1600\n",
      "Iteration 1248, loss = 0.24351106\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1249/1600\n",
      "Iteration 1249, loss = 0.24342221\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1250/1600\n",
      "Iteration 1250, loss = 0.24347359\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1251/1600\n",
      "Iteration 1251, loss = 0.24338780\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1252/1600\n",
      "Iteration 1252, loss = 0.24343893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1253/1600\n",
      "Iteration 1253, loss = 0.24336015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1254/1600\n",
      "Iteration 1254, loss = 0.24340939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1255/1600\n",
      "Iteration 1255, loss = 0.24333358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1256/1600\n",
      "Iteration 1256, loss = 0.24338489\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1257/1600\n",
      "Iteration 1257, loss = 0.24331400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1258/1600\n",
      "Iteration 1258, loss = 0.24337182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1259/1600\n",
      "Iteration 1259, loss = 0.24331067\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1260/1600\n",
      "Iteration 1260, loss = 0.24336698\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1261/1600\n",
      "Iteration 1261, loss = 0.24330375\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1262/1600\n",
      "Iteration 1262, loss = 0.24336411\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1263/1600\n",
      "Iteration 1263, loss = 0.24330910\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1264/1600\n",
      "Iteration 1264, loss = 0.24338234\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1265/1600\n",
      "Iteration 1265, loss = 0.24332982\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1266/1600\n",
      "Iteration 1266, loss = 0.24339994\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1267/1600\n",
      "Iteration 1267, loss = 0.24334913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1268/1600\n",
      "Iteration 1268, loss = 0.24342539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1269/1600\n",
      "Iteration 1269, loss = 0.24336464\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1270/1600\n",
      "Iteration 1270, loss = 0.24344212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1271/1600\n",
      "Iteration 1271, loss = 0.24338979\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1272/1600\n",
      "Iteration 1272, loss = 0.24347118\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1273/1600\n",
      "Iteration 1273, loss = 0.24340826\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1274/1600\n",
      "Iteration 1274, loss = 0.24349008\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1275/1600\n",
      "Iteration 1275, loss = 0.24342785\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1276/1600\n",
      "Iteration 1276, loss = 0.24351797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1277/1600\n",
      "Iteration 1277, loss = 0.24345702\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1278/1600\n",
      "Iteration 1278, loss = 0.24355222\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1279/1600\n",
      "Iteration 1279, loss = 0.24348641\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1280/1600\n",
      "Iteration 1280, loss = 0.24358989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1281/1600\n",
      "Iteration 1281, loss = 0.24352538\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1282/1600\n",
      "Iteration 1282, loss = 0.24363515\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1283/1600\n",
      "Iteration 1283, loss = 0.24355576\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1284/1600\n",
      "Iteration 1284, loss = 0.24367057\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1285/1600\n",
      "Iteration 1285, loss = 0.24359243\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1286/1600\n",
      "Iteration 1286, loss = 0.24371313\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1287/1600\n",
      "Iteration 1287, loss = 0.24363262\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1288/1600\n",
      "Iteration 1288, loss = 0.24375989\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1289/1600\n",
      "Iteration 1289, loss = 0.24370249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1290/1600\n",
      "Iteration 1290, loss = 0.24384444\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1291/1600\n",
      "Iteration 1291, loss = 0.24374622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1292/1600\n",
      "Iteration 1292, loss = 0.24388686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1293/1600\n",
      "Iteration 1293, loss = 0.24379573\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1294/1600\n",
      "Iteration 1294, loss = 0.24394686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1295/1600\n",
      "Iteration 1295, loss = 0.24382890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1296/1600\n",
      "Iteration 1296, loss = 0.24397964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1297/1600\n",
      "Iteration 1297, loss = 0.24386848\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1298/1600\n",
      "Iteration 1298, loss = 0.24402302\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1299/1600\n",
      "Iteration 1299, loss = 0.24389598\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1300/1600\n",
      "Iteration 1300, loss = 0.24405924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1301/1600\n",
      "Iteration 1301, loss = 0.24392307\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1302/1600\n",
      "Iteration 1302, loss = 0.24408225\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1303/1600\n",
      "Iteration 1303, loss = 0.24394535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1304/1600\n",
      "Iteration 1304, loss = 0.24410007\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1305/1600\n",
      "Iteration 1305, loss = 0.24395430\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1306/1600\n",
      "Iteration 1306, loss = 0.24410957\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1307/1600\n",
      "Iteration 1307, loss = 0.24396018\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1308/1600\n",
      "Iteration 1308, loss = 0.24410978\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1309/1600\n",
      "Iteration 1309, loss = 0.24395601\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1310/1600\n",
      "Iteration 1310, loss = 0.24410493\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1311/1600\n",
      "Iteration 1311, loss = 0.24397481\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1312/1600\n",
      "Iteration 1312, loss = 0.24413457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1313/1600\n",
      "Iteration 1313, loss = 0.24399010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1314/1600\n",
      "Iteration 1314, loss = 0.24414348\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1315/1600\n",
      "Iteration 1315, loss = 0.24397824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1316/1600\n",
      "Iteration 1316, loss = 0.24412206\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1317/1600\n",
      "Iteration 1317, loss = 0.24396045\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1318/1600\n",
      "Iteration 1318, loss = 0.24409211\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1319/1600\n",
      "Iteration 1319, loss = 0.24392805\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1320/1600\n",
      "Iteration 1320, loss = 0.24405549\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1321/1600\n",
      "Iteration 1321, loss = 0.24389592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1322/1600\n",
      "Iteration 1322, loss = 0.24402133\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1323/1600\n",
      "Iteration 1323, loss = 0.24386711\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1324/1600\n",
      "Iteration 1324, loss = 0.24398677\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1325/1600\n",
      "Iteration 1325, loss = 0.24384141\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1326/1600\n",
      "Iteration 1326, loss = 0.24395971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1327/1600\n",
      "Iteration 1327, loss = 0.24382212\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1328/1600\n",
      "Iteration 1328, loss = 0.24394166\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1329/1600\n",
      "Iteration 1329, loss = 0.24379452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1330/1600\n",
      "Iteration 1330, loss = 0.24390607\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1331/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1331, loss = 0.24376361\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1332/1600\n",
      "Iteration 1332, loss = 0.24386778\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1333/1600\n",
      "Iteration 1333, loss = 0.24372983\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1334/1600\n",
      "Iteration 1334, loss = 0.24382144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1335/1600\n",
      "Iteration 1335, loss = 0.24371429\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1336/1600\n",
      "Iteration 1336, loss = 0.24380706\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1337/1600\n",
      "Iteration 1337, loss = 0.24367105\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1338/1600\n",
      "Iteration 1338, loss = 0.24376705\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1339/1600\n",
      "Iteration 1339, loss = 0.24363837\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1340/1600\n",
      "Iteration 1340, loss = 0.24373085\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1341/1600\n",
      "Iteration 1341, loss = 0.24361061\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1342/1600\n",
      "Iteration 1342, loss = 0.24369329\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1343/1600\n",
      "Iteration 1343, loss = 0.24357626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1344/1600\n",
      "Iteration 1344, loss = 0.24366659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1345/1600\n",
      "Iteration 1345, loss = 0.24355680\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1346/1600\n",
      "Iteration 1346, loss = 0.24363640\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1347/1600\n",
      "Iteration 1347, loss = 0.24353508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1348/1600\n",
      "Iteration 1348, loss = 0.24360885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1349/1600\n",
      "Iteration 1349, loss = 0.24351539\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1350/1600\n",
      "Iteration 1350, loss = 0.24360383\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1351/1600\n",
      "Iteration 1351, loss = 0.24350289\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1352/1600\n",
      "Iteration 1352, loss = 0.24358017\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1353/1600\n",
      "Iteration 1353, loss = 0.24348301\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1354/1600\n",
      "Iteration 1354, loss = 0.24355612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1355/1600\n",
      "Iteration 1355, loss = 0.24346386\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1356/1600\n",
      "Iteration 1356, loss = 0.24355154\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1357/1600\n",
      "Iteration 1357, loss = 0.24345947\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1358/1600\n",
      "Iteration 1358, loss = 0.24353225\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1359/1600\n",
      "Iteration 1359, loss = 0.24344213\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1360/1600\n",
      "Iteration 1360, loss = 0.24351801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1361/1600\n",
      "Iteration 1361, loss = 0.24343909\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1362/1600\n",
      "Iteration 1362, loss = 0.24351770\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1363/1600\n",
      "Iteration 1363, loss = 0.24343110\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1364/1600\n",
      "Iteration 1364, loss = 0.24350540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1365/1600\n",
      "Iteration 1365, loss = 0.24342087\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1366/1600\n",
      "Iteration 1366, loss = 0.24348937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1367/1600\n",
      "Iteration 1367, loss = 0.24340645\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1368/1600\n",
      "Iteration 1368, loss = 0.24347995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1369/1600\n",
      "Iteration 1369, loss = 0.24340018\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1370/1600\n",
      "Iteration 1370, loss = 0.24347864\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1371/1600\n",
      "Iteration 1371, loss = 0.24339952\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1372/1600\n",
      "Iteration 1372, loss = 0.24347128\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1373/1600\n",
      "Iteration 1373, loss = 0.24339624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1374/1600\n",
      "Iteration 1374, loss = 0.24347191\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1375/1600\n",
      "Iteration 1375, loss = 0.24340713\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1376/1600\n",
      "Iteration 1376, loss = 0.24349685\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1377/1600\n",
      "Iteration 1377, loss = 0.24343279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1378/1600\n",
      "Iteration 1378, loss = 0.24352267\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1379/1600\n",
      "Iteration 1379, loss = 0.24344210\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1380/1600\n",
      "Iteration 1380, loss = 0.24353167\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1381/1600\n",
      "Iteration 1381, loss = 0.24345000\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1382/1600\n",
      "Iteration 1382, loss = 0.24353669\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1383/1600\n",
      "Iteration 1383, loss = 0.24345457\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1384/1600\n",
      "Iteration 1384, loss = 0.24354769\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1385/1600\n",
      "Iteration 1385, loss = 0.24346594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1386/1600\n",
      "Iteration 1386, loss = 0.24355734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1387/1600\n",
      "Iteration 1387, loss = 0.24348507\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1388/1600\n",
      "Iteration 1388, loss = 0.24358142\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1389/1600\n",
      "Iteration 1389, loss = 0.24349830\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1390/1600\n",
      "Iteration 1390, loss = 0.24359907\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1391/1600\n",
      "Iteration 1391, loss = 0.24351233\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1392/1600\n",
      "Iteration 1392, loss = 0.24361279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1393/1600\n",
      "Iteration 1393, loss = 0.24353010\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1394/1600\n",
      "Iteration 1394, loss = 0.24363801\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1395/1600\n",
      "Iteration 1395, loss = 0.24356797\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1396/1600\n",
      "Iteration 1396, loss = 0.24367665\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1397/1600\n",
      "Iteration 1397, loss = 0.24359158\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1398/1600\n",
      "Iteration 1398, loss = 0.24370903\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1399/1600\n",
      "Iteration 1399, loss = 0.24360924\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1400/1600\n",
      "Iteration 1400, loss = 0.24372930\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1401/1600\n",
      "Iteration 1401, loss = 0.24362755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1402/1600\n",
      "Iteration 1402, loss = 0.24374527\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1403/1600\n",
      "Iteration 1403, loss = 0.24363898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1404/1600\n",
      "Iteration 1404, loss = 0.24375795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1405/1600\n",
      "Iteration 1405, loss = 0.24365279\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1406/1600\n",
      "Iteration 1406, loss = 0.24376672\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1407/1600\n",
      "Iteration 1407, loss = 0.24365543\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1408/1600\n",
      "Iteration 1408, loss = 0.24377070\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1409/1600\n",
      "Iteration 1409, loss = 0.24367033\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1410/1600\n",
      "Iteration 1410, loss = 0.24379406\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1411/1600\n",
      "Iteration 1411, loss = 0.24369427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1412/1600\n",
      "Iteration 1412, loss = 0.24383125\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1413/1600\n",
      "Iteration 1413, loss = 0.24371229\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1414/1600\n",
      "Iteration 1414, loss = 0.24383311\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1415/1600\n",
      "Iteration 1415, loss = 0.24371552\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1416/1600\n",
      "Iteration 1416, loss = 0.24383510\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1417/1600\n",
      "Iteration 1417, loss = 0.24370626\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1418/1600\n",
      "Iteration 1418, loss = 0.24381632\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1419/1600\n",
      "Iteration 1419, loss = 0.24368971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1420/1600\n",
      "Iteration 1420, loss = 0.24379522\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1421/1600\n",
      "Iteration 1421, loss = 0.24368378\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1422/1600\n",
      "Iteration 1422, loss = 0.24379414\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1423/1600\n",
      "Iteration 1423, loss = 0.24367130\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1424/1600\n",
      "Iteration 1424, loss = 0.24377441\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1425/1600\n",
      "Iteration 1425, loss = 0.24365227\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1426/1600\n",
      "Iteration 1426, loss = 0.24375169\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1427/1600\n",
      "Iteration 1427, loss = 0.24363144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1428/1600\n",
      "Iteration 1428, loss = 0.24372771\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1429/1600\n",
      "Iteration 1429, loss = 0.24361240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1430/1600\n",
      "Iteration 1430, loss = 0.24370795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1431/1600\n",
      "Iteration 1431, loss = 0.24359251\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1432/1600\n",
      "Iteration 1432, loss = 0.24368621\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1433/1600\n",
      "Iteration 1433, loss = 0.24357686\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1434/1600\n",
      "Iteration 1434, loss = 0.24367608\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1435/1600\n",
      "Iteration 1435, loss = 0.24356833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1436/1600\n",
      "Iteration 1436, loss = 0.24367232\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1437/1600\n",
      "Iteration 1437, loss = 0.24356132\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1438/1600\n",
      "Iteration 1438, loss = 0.24365540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1439/1600\n",
      "Iteration 1439, loss = 0.24355760\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1440/1600\n",
      "Iteration 1440, loss = 0.24365372\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1441/1600\n",
      "Iteration 1441, loss = 0.24354334\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1442/1600\n",
      "Iteration 1442, loss = 0.24362936\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1443/1600\n",
      "Iteration 1443, loss = 0.24352205\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1444/1600\n",
      "Iteration 1444, loss = 0.24360335\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1445/1600\n",
      "Iteration 1445, loss = 0.24350216\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1446/1600\n",
      "Iteration 1446, loss = 0.24357952\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1447/1600\n",
      "Iteration 1447, loss = 0.24347964\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1448/1600\n",
      "Iteration 1448, loss = 0.24355590\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1449/1600\n",
      "Iteration 1449, loss = 0.24345790\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1450/1600\n",
      "Iteration 1450, loss = 0.24353080\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1451/1600\n",
      "Iteration 1451, loss = 0.24343840\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1452/1600\n",
      "Iteration 1452, loss = 0.24351993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1453/1600\n",
      "Iteration 1453, loss = 0.24343005\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1454/1600\n",
      "Iteration 1454, loss = 0.24350861\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1455/1600\n",
      "Iteration 1455, loss = 0.24341941\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1456/1600\n",
      "Iteration 1456, loss = 0.24349240\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1457/1600\n",
      "Iteration 1457, loss = 0.24342971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1458/1600\n",
      "Iteration 1458, loss = 0.24351345\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1459/1600\n",
      "Iteration 1459, loss = 0.24347081\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1460/1600\n",
      "Iteration 1460, loss = 0.24355306\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1461/1600\n",
      "Iteration 1461, loss = 0.24344939\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1462/1600\n",
      "Iteration 1462, loss = 0.24351524\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1463/1600\n",
      "Iteration 1463, loss = 0.24342181\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1464/1600\n",
      "Iteration 1464, loss = 0.24348734\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1465/1600\n",
      "Iteration 1465, loss = 0.24340242\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1466/1600\n",
      "Iteration 1466, loss = 0.24347193\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1467/1600\n",
      "Iteration 1467, loss = 0.24338599\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1468/1600\n",
      "Iteration 1468, loss = 0.24345853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1469/1600\n",
      "Iteration 1469, loss = 0.24337655\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1470/1600\n",
      "Iteration 1470, loss = 0.24344639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1471/1600\n",
      "Iteration 1471, loss = 0.24336755\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1472/1600\n",
      "Iteration 1472, loss = 0.24344330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1473/1600\n",
      "Iteration 1473, loss = 0.24336757\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1474/1600\n",
      "Iteration 1474, loss = 0.24344554\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1475/1600\n",
      "Iteration 1475, loss = 0.24336612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1476/1600\n",
      "Iteration 1476, loss = 0.24344971\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1477/1600\n",
      "Iteration 1477, loss = 0.24337012\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1478/1600\n",
      "Iteration 1478, loss = 0.24344726\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1479/1600\n",
      "Iteration 1479, loss = 0.24336762\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1480/1600\n",
      "Iteration 1480, loss = 0.24344518\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1481/1600\n",
      "Iteration 1481, loss = 0.24336613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1482/1600\n",
      "Iteration 1482, loss = 0.24344095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1483/1600\n",
      "Iteration 1483, loss = 0.24337929\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1484/1600\n",
      "Iteration 1484, loss = 0.24346484\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1485/1600\n",
      "Iteration 1485, loss = 0.24338600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1486/1600\n",
      "Iteration 1486, loss = 0.24346937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1487/1600\n",
      "Iteration 1487, loss = 0.24338958\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1488/1600\n",
      "Iteration 1488, loss = 0.24347502\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1489/1600\n",
      "Iteration 1489, loss = 0.24341447\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1490/1600\n",
      "Iteration 1490, loss = 0.24350680\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1491/1600\n",
      "Iteration 1491, loss = 0.24343204\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1492/1600\n",
      "Iteration 1492, loss = 0.24353058\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1493/1600\n",
      "Iteration 1493, loss = 0.24344824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1494/1600\n",
      "Iteration 1494, loss = 0.24353890\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1495/1600\n",
      "Iteration 1495, loss = 0.24345391\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1496/1600\n",
      "Iteration 1496, loss = 0.24354579\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1497/1600\n",
      "Iteration 1497, loss = 0.24345993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1498/1600\n",
      "Iteration 1498, loss = 0.24355316\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1499/1600\n",
      "Iteration 1499, loss = 0.24346661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1500/1600\n",
      "Iteration 1500, loss = 0.24356204\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1501/1600\n",
      "Iteration 1501, loss = 0.24347662\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1502/1600\n",
      "Iteration 1502, loss = 0.24357290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1503/1600\n",
      "Iteration 1503, loss = 0.24348450\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1504/1600\n",
      "Iteration 1504, loss = 0.24358065\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1505/1600\n",
      "Iteration 1505, loss = 0.24348894\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1506/1600\n",
      "Iteration 1506, loss = 0.24358515\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1507/1600\n",
      "Iteration 1507, loss = 0.24350582\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1508/1600\n",
      "Iteration 1508, loss = 0.24360833\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1509/1600\n",
      "Iteration 1509, loss = 0.24351241\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1510/1600\n",
      "Iteration 1510, loss = 0.24360836\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1511/1600\n",
      "Iteration 1511, loss = 0.24351358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1512/1600\n",
      "Iteration 1512, loss = 0.24361051\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1513/1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1513, loss = 0.24351670\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1514/1600\n",
      "Iteration 1514, loss = 0.24361619\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1515/1600\n",
      "Iteration 1515, loss = 0.24354699\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1516/1600\n",
      "Iteration 1516, loss = 0.24365967\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1517/1600\n",
      "Iteration 1517, loss = 0.24359402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1518/1600\n",
      "Iteration 1518, loss = 0.24371415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1519/1600\n",
      "Iteration 1519, loss = 0.24360434\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1520/1600\n",
      "Iteration 1520, loss = 0.24375611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1521/1600\n",
      "Iteration 1521, loss = 0.24364417\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1522/1600\n",
      "Iteration 1522, loss = 0.24375821\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1523/1600\n",
      "Iteration 1523, loss = 0.24363812\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1524/1600\n",
      "Iteration 1524, loss = 0.24374995\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1525/1600\n",
      "Iteration 1525, loss = 0.24362946\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1526/1600\n",
      "Iteration 1526, loss = 0.24373694\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1527/1600\n",
      "Iteration 1527, loss = 0.24361400\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1528/1600\n",
      "Iteration 1528, loss = 0.24371661\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1529/1600\n",
      "Iteration 1529, loss = 0.24359748\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1530/1600\n",
      "Iteration 1530, loss = 0.24369548\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1531/1600\n",
      "Iteration 1531, loss = 0.24357555\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1532/1600\n",
      "Iteration 1532, loss = 0.24366312\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1533/1600\n",
      "Iteration 1533, loss = 0.24354737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1534/1600\n",
      "Iteration 1534, loss = 0.24362915\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1535/1600\n",
      "Iteration 1535, loss = 0.24352913\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1536/1600\n",
      "Iteration 1536, loss = 0.24362110\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1537/1600\n",
      "Iteration 1537, loss = 0.24350893\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1538/1600\n",
      "Iteration 1538, loss = 0.24358642\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1539/1600\n",
      "Iteration 1539, loss = 0.24347854\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1540/1600\n",
      "Iteration 1540, loss = 0.24355415\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1541/1600\n",
      "Iteration 1541, loss = 0.24345120\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1542/1600\n",
      "Iteration 1542, loss = 0.24352435\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1543/1600\n",
      "Iteration 1543, loss = 0.24342547\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1544/1600\n",
      "Iteration 1544, loss = 0.24349483\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1545/1600\n",
      "Iteration 1545, loss = 0.24340358\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1546/1600\n",
      "Iteration 1546, loss = 0.24347602\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1547/1600\n",
      "Iteration 1547, loss = 0.24338219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1548/1600\n",
      "Iteration 1548, loss = 0.24344808\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1549/1600\n",
      "Iteration 1549, loss = 0.24335846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1550/1600\n",
      "Iteration 1550, loss = 0.24341783\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1551/1600\n",
      "Iteration 1551, loss = 0.24333144\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1552/1600\n",
      "Iteration 1552, loss = 0.24339541\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1553/1600\n",
      "Iteration 1553, loss = 0.24331249\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1554/1600\n",
      "Iteration 1554, loss = 0.24337727\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1555/1600\n",
      "Iteration 1555, loss = 0.24329643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1556/1600\n",
      "Iteration 1556, loss = 0.24335934\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1557/1600\n",
      "Iteration 1557, loss = 0.24328611\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1558/1600\n",
      "Iteration 1558, loss = 0.24335228\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1559/1600\n",
      "Iteration 1559, loss = 0.24328550\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1560/1600\n",
      "Iteration 1560, loss = 0.24335425\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1561/1600\n",
      "Iteration 1561, loss = 0.24328021\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1562/1600\n",
      "Iteration 1562, loss = 0.24334508\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1563/1600\n",
      "Iteration 1563, loss = 0.24326895\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1564/1600\n",
      "Iteration 1564, loss = 0.24332885\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1565/1600\n",
      "Iteration 1565, loss = 0.24325634\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1566/1600\n",
      "Iteration 1566, loss = 0.24331648\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1567/1600\n",
      "Iteration 1567, loss = 0.24325002\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1568/1600\n",
      "Iteration 1568, loss = 0.24331540\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1569/1600\n",
      "Iteration 1569, loss = 0.24324720\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1570/1600\n",
      "Iteration 1570, loss = 0.24331095\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1571/1600\n",
      "Iteration 1571, loss = 0.24324494\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1572/1600\n",
      "Iteration 1572, loss = 0.24330943\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1573/1600\n",
      "Iteration 1573, loss = 0.24325529\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1574/1600\n",
      "Iteration 1574, loss = 0.24332381\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1575/1600\n",
      "Iteration 1575, loss = 0.24325544\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1576/1600\n",
      "Iteration 1576, loss = 0.24332330\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1577/1600\n",
      "Iteration 1577, loss = 0.24325587\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1578/1600\n",
      "Iteration 1578, loss = 0.24332535\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1579/1600\n",
      "Iteration 1579, loss = 0.24326113\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1580/1600\n",
      "Iteration 1580, loss = 0.24333073\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1581/1600\n",
      "Iteration 1581, loss = 0.24326431\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1582/1600\n",
      "Iteration 1582, loss = 0.24333654\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1583/1600\n",
      "Iteration 1583, loss = 0.24327337\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1584/1600\n",
      "Iteration 1584, loss = 0.24334882\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1585/1600\n",
      "Iteration 1585, loss = 0.24328616\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1586/1600\n",
      "Iteration 1586, loss = 0.24336471\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1587/1600\n",
      "Iteration 1587, loss = 0.24330399\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1588/1600\n",
      "Iteration 1588, loss = 0.24338108\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1589/1600\n",
      "Iteration 1589, loss = 0.24332047\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1590/1600\n",
      "Iteration 1590, loss = 0.24340405\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1591/1600\n",
      "Iteration 1591, loss = 0.24333331\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1592/1600\n",
      "Iteration 1592, loss = 0.24341438\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1593/1600\n",
      "Iteration 1593, loss = 0.24334763\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1594/1600\n",
      "Iteration 1594, loss = 0.24343285\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1595/1600\n",
      "Iteration 1595, loss = 0.24335560\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1596/1600\n",
      "Iteration 1596, loss = 0.24343577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1597/1600\n",
      "Iteration 1597, loss = 0.24336853\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1598/1600\n",
      "Iteration 1598, loss = 0.24347170\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1599/1600\n",
      "Iteration 1599, loss = 0.24340410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Training epoch 1600/1600\n",
      "Iteration 1600, loss = 0.24351015\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1600)\n",
    "history = clf.fit(X_train, y_train)\n",
    "\n",
    "# plot the loss curve\n",
    "plt.plot(history.loss_curve_)\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# # plot the accuracy curve\n",
    "# plt.plot(history.score(X_train, y_train))\n",
    "# plt.plot(history.score(X_test, y_test))\n",
    "# plt.title('Accuracy Curves')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(['Train', 'Test'])\n",
    "# plt.show()\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1600, alpha=0.0001, solver='sgd', verbose=10, random_state=1, learning_rate_init=.1)\n",
    "n_epochs = 1600\n",
    "train_accs = np.zeros(n_epochs)\n",
    "val_accs = np.zeros(n_epochs)\n",
    "train_losses = np.zeros(n_epochs)\n",
    "val_losses = np.zeros(n_epochs)\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    print(f\"Training epoch {i+1}/{n_epochs}\")\n",
    "    # train the model on the training set for one epoch\n",
    "    clf.partial_fit(X_train, y_train, classes=np.unique(y_train))\n",
    "    \n",
    "    # compute the training and validation accuracies and losses\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    train_accs[i] = accuracy_score(y_train, y_train_pred)\n",
    "    val_accs[i] = accuracy_score(y_test, y_test_pred)\n",
    "    train_losses[i] = log_loss(y_train, y_train_pred)\n",
    "    val_losses[i] = log_loss(y_test, y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e0f36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
